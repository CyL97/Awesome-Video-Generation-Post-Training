# Conference / Journal Papers

| **Title (Conference / Journal)** | **Year** | **Paper** |
| --- | --- | :---: |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) PanFlow: Decoupled Motion Control for Panoramic Video Generation | 2026 | [Paper](https://arxiv.org/abs/2512.00832v1) · [GitHub](https://github.com/chengzhag/PanFlow) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Aligning What Matters: Masked Latent Adaptation for Text-to-Audio-Video Generation | 2025 | [Paper](https://neurips.cc/virtual/2025/loc/mexico-city/poster/118857) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Audio-Sync Video Generation with Multi-Stream Temporal Control | 2025 | [Paper](https://arxiv.org/abs/2506.08003v1) · [GitHub](https://github.com/suimuc/MTV_Framework) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.09350v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.03517v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) EchoShot: Multi-Shot Portrait Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.15838) · [GitHub](https://github.com/JoHnneyWang/EchoShot) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Frame Context Packing and Drift Prevention in Next-Frame-Prediction Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2504.12626) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Frame In-N-Out: Unbounded Controllable Image-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2505.21491v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) GeoVideo: Introducing Geometric Regularization into Video Generation Model | 2025 | [Paper](https://arxiv.org/abs/2512.03453v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.10858v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Imagine360: Immersive 360 Video Generation from Perspective Anchor | 2025 | [Paper](https://arxiv.org/abs/2412.03552) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Improving Video Generation with Human Feedback | 2025 | [Paper](https://arxiv.org/abs/2501.13918) · [GitHub](https://github.com/KlingTeam/VideoAlign) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) MoCha: Towards Movie-Grade Talking Character Generation | 2025 | [Paper](https://arxiv.org/abs/2503.23307) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement | 2025 | [Paper](https://arxiv.org/abs/2506.07848v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.19852v1) · [GitHub](https://github.com/mit-han-lab/radial-attention) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation | 2025 | [Paper](https://arxiv.org/abs/2509.16500v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.00996) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models | 2025 | [Paper](https://arxiv.org/abs/2505.23656v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image | 2025 | [Paper](https://arxiv.org/abs/2509.04450v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance | 2025 | [Paper](https://arxiv.org/abs/2512.08765) · [GitHub](https://github.com/ali-vilab/Wan-Move) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2503.08153) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception | 2025 | [Paper](https://arxiv.org/abs/2508.15720v1) |
| ![EMNLP](https://img.shields.io/badge/EMNLP-darkorange) VC4VG: Optimizing Video Captions for Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2510.24134) · [GitHub](https://github.com/qyr0403/VC4VG) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) AICL: Action In-Context Learning for Video Diffusion Model | 2025 | [Paper](https://dl.acm.org/doi/10.1145/3746027.3754864) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) Improving Identity Preservation in Video Generation with Multi-Branch Models | 2025 | [Paper](https://dl.acm.org/doi/10.1145/3746027.3761990) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) M2PE-DIFF: Music-to-Pose Encoder for Dance Video Generation Leveraging Latent Diffusion Framework. | 2025 | [Paper](https://doi.org/10.1145/3746027.3754808) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation | 2025 | [Paper](https://arxiv.org/html/2508.00782v1) · [GitHub](https://github.com/tkpham3105/SpA2V) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation | 2025 | [Paper](https://arxiv.org/abs/2507.05963) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction | 2025 | [Paper](https://arxiv.org/abs/2406.06465) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis | 2025 | [Paper](https://arxiv.org/abs/2507.18569) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Decouple and Track: Benchmarking and Improving Video Diffusion Transformers For Motion Transfer | 2025 | [Paper](https://arxiv.org/abs/2503.17350) · [GitHub](https://github.com/Shi-qingyu/DeT) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion | 2025 | [Paper](https://arxiv.org/abs/2504.04010) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) DOLLAR: Few-Step Video Generation via Distillation and Latent Reward Optimization | 2025 | [Paper](https://arxiv.org/abs/2412.15689) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Dual-Expert Consistency Model for Efficient and High-Quality Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.03123) · [GitHub](https://github.com/Vchitect/DCM) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization | 2025 | [Paper](https://arxiv.org/abs/2505.02192) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) EfficientMT: Efficient Temporal Adaptation for Motion Transfer in Text-to-Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2503.19369) · [GitHub](https://github.com/PrototypeNx/EfficientMT) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Free-Form Motion Control: Controlling the 6D Poses of Camera and Objects in Video Generation | 2025 | [Paper](https://arxiv.org/abs/2501.01425) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) I2VControl: Disentangled and Unified Video Motion Synthesis Control | 2025 | [Paper](https://arxiv.org/abs/2411.17765) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) LayerAnimate: Layer-level Control for Animation | 2025 | [Paper](https://arxiv.org/abs/2501.08295) · [GitHub](https://github.com/IamCreateAI/LayerAnimate) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Learning Few-Step Diffusion Models by Trajectory Distribution Matching | 2025 | [Paper](https://arxiv.org/abs/2503.06674) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2507.05678) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Long Context Tuning for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2503.10589) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization | 2025 | [Paper](https://arxiv.org/abs/2503.12689) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MagicMirror: ID-Preserved Video Generation in Video Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2501.03931) · [GitHub](https://github.com/dvlab-research/MagicMirror) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance | 2025 | [Paper](https://arxiv.org/abs/2503.16421) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Mobile Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2412.07583) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent | 2025 | [Paper](https://arxiv.org/abs/2502.03207) · [GitHub](https://github.com/leoisufa/MotionAgent) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) PersonalVideo: High ID-Fidelity Video Customization without Dynamic and Semantic Degradation | 2025 | [Paper](https://arxiv.org/abs/2411.17048) · [GitHub](https://github.com/EchoPluto/PersonalVideo?tab=readme-ov-file) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment | 2025 | [Paper](https://arxiv.org/abs/2502.11079) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Precise Action-to-Video Generation Through Visual Action Prompts | 2025 | [Paper](https://arxiv.org/abs/2508.13104) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned LLM | 2025 | [Paper](https://arxiv.org/abs/2412.15156) · [GitHub](https://github.com/jiyt17/Prompt-A-Video) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics | 2025 | [Paper](https://arxiv.org/abs/2408.04631) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Reangle-A-Video: 4D Video Generation as Video-to-Video Translation | 2025 | [Paper](https://arxiv.org/abs/2503.09151) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control | 2025 | [Paper](https://arxiv.org/abs/2502.10059) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution | 2025 | [Paper](https://arxiv.org/abs/2501.02976) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) TACO: Taming Diffusion for in-the-wild Video Amodal Completion | 2025 | [Paper](https://arxiv.org/abs/2503.12049) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) V.I.P.: Iterative Online Preference Distillation for Efficient Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2508.03254) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded Generation | 2025 | [Paper](https://arxiv.org/abs/2503.14350) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Versatile Transition Generation with Image-to-Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2508.01698) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) VideoAuteur: Towards Long Narrative Video Generation | 2025 | [Paper](https://arxiv.org/abs/2501.06173) |
| ![ECCV](https://img.shields.io/badge/ECCV-olive) Animate Your Motion: Turning Still Images into Dynamic Videos | 2024 | [Paper](https://arxiv.org/abs/2403.10179) |
| ![ECCV](https://img.shields.io/badge/ECCV-olive) DragVideo: Interactive Drag-style Video Editing | 2024 | [Paper](https://arxiv.org/abs/2312.02216) |
| ![ECCV](https://img.shields.io/badge/ECCV-olive) MEVG : Multi-event Video Generation with Text-to-Video Models | 2024 | [Paper](https://arxiv.org/abs/2312.04086) |
| ![ECCV](https://img.shields.io/badge/ECCV-olive) MoVideo: Motion-Aware Video Generation with Diffusion Models | 2024 | [Paper](https://arxiv.org/abs/2311.11325) |
| ![ECCV](https://img.shields.io/badge/ECCV-olive) SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models | 2024 | [Paper](https://arxiv.org/abs/2311.16933) |
| ![ECCV](https://img.shields.io/badge/ECCV-olive) Stable Video Portraits | 2024 | [Paper](https://arxiv.org/abs/2409.18083) |
| ![ECCV](https://img.shields.io/badge/ECCV-olive) TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models | 2024 | [Paper](https://arxiv.org/abs/2407.09012) |
| ![ECCV](https://img.shields.io/badge/ECCV-olive) Video Editing via Factorized Diffusion Distillation | 2024 | [Paper](https://arxiv.org/abs/2403.09334) |
| ![ECCV](https://img.shields.io/badge/ECCV-olive) VideoStudio: Generating Consistent-Content and Multi-Scene Videos | 2024 | [Paper](https://arxiv.org/abs/2401.01256) · [GitHub](https://github.com/FuchenUSTC/VideoStudio) |
| ![COLM](https://img.shields.io/badge/COLM-orange) VideoDirectorGPT: Consistent Multi-Scene Video Generation via LLM-Guided Planning | 2024 | [Paper](https://arxiv.org/abs/2309.15091) |
| ![IJCAI](https://img.shields.io/badge/IJCAI-royalblue) FLAP: Fully-controllable Audio-driven Portrait Video Generation through 3D head conditioned diffusion model | 2025 | [Paper](https://arxiv.org/html/2502.19455v3) |
| ![ICML](https://img.shields.io/badge/ICML-indigo) FrameBridge: Improving Image-to-Video Generation with Bridge Models | 2025 | [Paper](https://arxiv.org/abs/2410.15371) |
| ![PMLR](https://img.shields.io/badge/PMLR-gold) Diffusion Adversarial Post-Training for One-Step Video Generation  | 2025 | [Paper](https://arxiv.org/abs/2501.08316) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) AKiRa: Augmentation Kit on Rays for Optical Video Generation | 2025 | [Paper](https://arxiv.org/abs/2412.14158) · [GitHub](https://github.com/Triocrossing/AKiRa) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2503.08417) · [GitHub](https://github.com/kwanyun/AnyMoLe) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think | 2025 | [Paper](https://arxiv.org/abs/2503.00948) · [GitHub](https://github.com/Chuge0335/EDG) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations | 2025 | [Paper](https://arxiv.org/abs/2411.10818) · [GitHub](https://github.com/hmrishavbandy/FlipSketch) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) From Slow Bidirectional to Fast Autoregressive Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2412.07772) · [GitHub](https://github.com/tianweiy/CausVid) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise | 2025 | [Paper](https://arxiv.org/abs/2501.08331) · [GitHub](https://github.com/Eyeline-Research/Go-with-the-Flow) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) GS-DiT: Advancing Video Generation with Dynamic 3D Gaussian Fields through Efficient Dense 3D Point Tracking | 2025 | [Paper](https://arxiv.org/abs/2501.02690) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) High-Fidelity Relightable Monocular Portrait Animation with Lighting-Controllable Video Diffusion Model | 2025 | [Paper](https://arxiv.org/abs/2502.19894) · [GitHub](https://github.com/MingtaoGuo/Relightable-Portrait-Animation) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation | 2025 | [Paper](https://arxiv.org/abs/2503.18860) · [GitHub](https://github.com/kkakkkka/HunyuanPortrait) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) InterDyn: Controllable Interactive Dynamics with Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2412.11785) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) Lux Post Facto: Learning Portrait Performance Relighting with Conditional Video Diffusion and a Hybrid Dataset | 2025 | [Paper](https://arxiv.org/abs/2503.14485) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) Mind the Time: Temporally-Controlled Multi-Event Video Generation | 2025 | [Paper](https://arxiv.org/abs/2412.05263) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) MotiF: Making Text Count in Image Animation with Motion Focal Loss | 2025 | [Paper](https://arxiv.org/abs/2412.16153) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) One-Minute Video Generation with Test-Time Training | 2025 | [Paper](https://arxiv.org/abs/2504.05298) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning | 2025 | [Paper](https://arxiv.org/abs/2411.05003) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2505.07652) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) TASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation | 2025 | [Paper](https://arxiv.org/abs/2503.11423) · [GitHub](https://github.com/GAP-LAB-CUHK-SZ/TASTE-Rob) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) TransPixeler: Advancing Text-to-Video Generation with Transparency | 2025 | [Paper](https://arxiv.org/abs/2501.03006) · [GitHub](https://github.com/wileewang/TransPixeler) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) VideoDPO: Omni-Preference Alignment for Video Diffusion Generation | 2025 | [Paper](https://arxiv.org/abs/2412.14167) · [GitHub](https://github.com/CIntellifusion/VideoDPO) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) Boosting Camera Motion Control for Video Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2410.10802) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) CameraCtrl: Enabling Camera Control for Video Diffusion Models | 2025 | [Paper](https://arxiv.org/html/2404.02101v2) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model | 2025 | [Paper](https://arxiv.org/abs/2404.09967) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2505.11245) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) SePPO: Semi-Policy Preference Optimization for Diffusion Alignment | 2025 | [Paper](https://arxiv.org/html/2410.05255v1) · [GitHub](https://github.com/DwanZhang-AI/SePPO) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) Trajectory attention for fine-grained video motion control | 2025 | [Paper](https://arxiv.org/abs/2411.19324) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) VADER: Video Diffusion Alignment via Reward Gradients | 2025 | [Paper](https://arxiv.org/abs/2407.08737) · [GitHub](https://github.com/vader-vid/vader) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control | 2025 | [Paper](https://arxiv.org/abs/2407.12781) |
| ![WACV](https://img.shields.io/badge/WACV-dodgerblue) Fine-Grained Controllable Video Generation via Object Appearance and Context | 2025 | [Paper](https://arxiv.org/abs/2312.02919) |
| ![WACV](https://img.shields.io/badge/WACV-dodgerblue) MagicStick: Controllable Video Editing via Control Handle Transformations | 2025 | [Paper](https://arxiv.org/abs/2312.03047) |
| ![WACV](https://img.shields.io/badge/WACV-dodgerblue) TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2312.00651) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) CAGE:Unsupervised Visual Composition and Animation for Controllable Video Generation | 2025 | [Paper](https://arxiv.org/abs/2403.14368) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities | 2025 | [Paper](https://arxiv.org/abs/2408.13239) · [GitHub](https://github.com/WuTao-CS/CustomCrafter) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) CustomTTT: Motion and Appearance Customized Video Generation via Test-Time Training | 2025 | [Paper](https://arxiv.org/abs/2412.15646) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation | 2025 | [Paper](https://arxiv.org/abs/2403.06845) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) Enhancing Identity-Deformation Disentanglement in StyleGAN for One-Shot Face Video Re-Enactment | 2025 | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/32113) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM | 2025 | [Paper](https://arxiv.org/abs/2504.12048v1) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) Occlusion-Insensitive Talking Head Video Generation via Facelet Compensation | 2025 | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/32277) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) PanoDiT: Panoramic Videos Generation with Diffusion Transformer | 2025 | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/33089) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) TrackGo: A Flexible and Efficient Method for Controllable Video Generation | 2025 | [Paper](https://arxiv.org/abs/2408.11475) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) UFO: Enhancing Diffusion-Based Video Generation with a Uniform Frame Organizer | 2025 | [Paper](https://arxiv.org/abs/2412.09389) · [GitHub](https://github.com/Delong-liu-bupt/UFO) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Exo2Ego-V: Exocentric-to-Egocentric Video Generation | 2024 | [Paper](https://proceedings.neurips.cc/paper_files/paper/2024/hash/f5a8b5e5d007e66c929b971c2bc21d76-Abstract-Conference.html) · [GitHub](https://github.com/showlab/Exo2Ego-V) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) MotionBooth: Motion-Aware Customized Text-to-Video Generation | 2024 | [Paper](https://arxiv.org/abs/2406.17758) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) SF-V: Single Forward Video Generation Model | 2024 | [Paper](https://arxiv.org/abs/2406.04324) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback | 2024 | [Paper](https://arxiv.org/abs/2405.18750) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Vivid-ZOO: Multi-View Video Generation with Diffusion Model | 2024 | [Paper](https://arxiv.org/abs/2406.08659) |
| ![EMNLP](https://img.shields.io/badge/EMNLP-darkorange) VIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation | 2024 | [Paper](https://arxiv.org/abs/2406.15252) · [GitHub](https://github.com/TIGER-AI-Lab/VideoScore/) |
| ![EMNLP](https://img.shields.io/badge/EMNLP-darkorange) VIMI: Grounding Video Generation through Multi-modal Instruction | 2024 | [Paper](https://arxiv.org/abs/2407.06304) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) DisenStudio: Customized Multi-subject Text-to-Video Generation with Disentangled Spatial Control | 2024 | [Paper](https://arxiv.org/abs/2405.12796) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) DragEntity:Trajectory Guided Video Generation using Entity and Positional Relationships | 2024 | [Paper](https://arxiv.org/abs/2410.10751) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition | 2024 | [Paper](https://arxiv.org/abs/2403.14148) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction | 2024 | [Paper](https://arxiv.org/abs/2310.20700) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) VersVideo: Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation | 2024 | [Paper](https://openreview.net/forum?id=K9sVJ17zvB) · [Website](https://jinxixiang.github.io/versvideo/) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) InstructVideo: Instructing Video Diffusion Models with Human Feedback | 2024 | [Paper](https://arxiv.org/abs/2312.12490) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) SimDA: Simple Diffusion Adapter for Efficient Video Generation | 2024 | [Paper](https://arxiv.org/abs/2308.09710) · [GitHub](https://github.com/ChenHsing/SimDA) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models | 2024 | [Paper](https://arxiv.org/abs/2401.09047) · [GitHub](https://github.com/AILab-CVC/VideoCrafter) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) Follow Your Pose: Pose-Guided Text-to-Video Generation Using Pose-Free Videos | 2024 | [Paper](https://arxiv.org/abs/2304.01186) · [GitHub](https://github.com/mayuelala/FollowYourPose) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) VideoComposer: Compositional Video Synthesis with Motion Controllability | 2023 | [Paper](https://arxiv.org/abs/2306.02018) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) MobileVidFactory: Automatic Diffusion-Based Social Media Video Generation for Mobile Devices from Text | 2023 | [Paper](https://arxiv.org/abs/2307.16371) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) DreamPose: Fashion Video Synthesis with Stable Diffusion | 2023 | [Paper](https://arxiv.org/abs/2304.06025) · [GitHub](https://github.com/johannakarras/DreamPose) · [Website](https://grail.cs.washington.edu/projects/dreampose/)|
| ![ICCV](https://img.shields.io/badge/ICCV-green) Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models | 2023 | [Paper](https://arxiv.org/abs/2305.10474) · [Website](https://research.nvidia.com/labs/dir/pyoco/) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Structure and Content-Guided Video Synthesis with Diffusion Models | 2023 | [Paper](https://arxiv.org/abs/2302.03011) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation | 2023 | [Paper](https://arxiv.org/abs/2212.11565) · [GitHub](https://github.com/showlab/Tune-A-Video) |
