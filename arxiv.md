# arXiv Papers

This page contains arXiv papers related to video generation post-training.

---

| **Title** | **Year** | **Paper** |
| --- | --- | :---: |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation | 2025 | [Paper](https://arxiv.org/abs/2508.07981v3) · [Website](https://amap-ml.github.io/Omni-Effects.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2508.07149v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment | 2025 | [Paper](https://arxiv.org/abs/2508.06082v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DreamVE: Unified Instruction-based Image and Video Editing | 2025 | [Paper](https://arxiv.org/abs/2508.06080v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.05091v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.04228v1) · [Website](https://kr-panghu.github.io/LayerT2V/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Scaling Up Audio-Synchronized Visual Animation: An Efficient Training Paradigm | 2025 | [Paper](https://arxiv.org/abs/2508.03955v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.03694v1) · [Website](https://vchitect.github.io/LongVie-project/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.03334v3) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework | 2025 | [Paper](https://arxiv.org/abs/2508.02807v1) · [Website](https://virtu-lab.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Compositional Video Synthesis by Temporal Object-Centric Learning | 2025 | [Paper](https://arxiv.org/abs/2507.20855v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) AnimeColor: Reference-based Animation Colorization with Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2507.20158v1) · [GitHub](https://github.com/IamCreateAI/AnimeColor) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Enhancing Scene Transition Awareness in Video Generation via Post-Training | 2025 | [Paper](https://arxiv.org/abs/2507.18046v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA | 2025 | [Paper](https://arxiv.org/abs/2507.17963v1) · [Website](https://snap-research.github.io/zero-shot-dynamic-concepts/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with Video Diffusion Model | 2025 | [Paper](https://arxiv.org/abs/2507.16341v1) · [GitHub](https://github.com/MingtaoGuo/Face-Reenactment-Video-Diffusion) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation | 2025 | [Paper](https://arxiv.org/abs/2507.16116v1) · [Website](https://yaofang-liu.github.io/Pusa_Web/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Conditional Video Generation for High-Efficiency Video Compression | 2025 | [Paper](https://arxiv.org/abs/2507.15269v4) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation | 2025 | [Paper](https://arxiv.org/abs/2507.15064v1) · [Website](https://francis-rings.github.io/StableAnimator/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2507.13344v1) · [Website](https://diffuman4d.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling | 2025 | [Paper](https://arxiv.org/abs/2507.07982v1) · [Website](https://GeometryForcing.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Geometry-aware 4D Video Generation for Robot Manipulation | 2025 | [Paper](https://arxiv.org/abs/2507.01099v1) · [GitHub](https://github.com/lzylucy/4dgen) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Populate-A-Scene: Affordance-Aware Human Video Generation | 2025 | [Paper](https://arxiv.org/abs/2507.00334v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) TextMesh4D: High-Quality Text-to-4D Mesh Generation | 2025 | [Paper](https://arxiv.org/abs/2506.24121v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.23690v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis | 2025 | [Paper](https://arxiv.org/abs/2506.23263v1) · [Website](http://lotvsmmau.net/Causal-VidSyn) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) RoboScape: Physics-informed Embodied World Model | 2025 | [Paper](https://arxiv.org/abs/2506.23135v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy | 2025 | [Paper](https://arxiv.org/abs/2506.22432v2) · [Website](https://shapeformotion.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation | 2025 | [Paper](https://arxiv.org/abs/2506.22065v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) FairyGen: Storied Cartoon Video from a Single Child-Drawn Character | 2025 | [Paper](https://arxiv.org/abs/2506.21272v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Video Virtual Try-on with Conditional Diffusion Transformer Inpainter | 2025 | [Paper](https://arxiv.org/abs/2506.21270v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing | 2025 | [Paper](https://arxiv.org/abs/2506.20967v2) · [Website](https://dfvedit.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.19851v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Bind-Your-Avatar: Multi-Talking-Character Video Generation with Dynamic 3D-mask-based Embedding Router | 2025 | [Paper](https://arxiv.org/abs/2506.19833v1) · [GitHub](https://github.com/Yubo-Shankui/Bind-Your-Avatar-Implementation) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) RDPO: Real Data Preference Optimization for Physics Consistency Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.18655v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) FramePrompt: In-context Controllable Animation with Zero Structural Changes | 2025 | [Paper](https://arxiv.org/abs/2506.17301v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Causally Steered Diffusion for Automated Video Counterfactual Generation | 2025 | [Paper](https://arxiv.org/abs/2506.14404v2) · [GitHub](https://github.com/nysp78/counterfactual-video-generation) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer | 2025 | [Paper](https://arxiv.org/abs/2506.12847v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning | 2025 | [Paper](https://arxiv.org/abs/2506.14827v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning | 2025 | [Paper](https://arxiv.org/abs/2506.10639v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2506.10568v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning | 2025 | [Paper](https://arxiv.org/abs/2506.10082v5) · [GitHub](https://github.com/cjeen/LoRAEdit) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PlayerOne: Egocentric World Simulator | 2025 | [Paper](https://arxiv.org/abs/2506.09995v1) · [GitHub](https://github.com/yuanpengtu/PlayerOne) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation | 2025 | [Paper](https://arxiv.org/abs/2506.11144v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.09229v2) · [GitHub](https://github.com/deepshwang/crepa) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models | 2025 | [Paper](https://arxiv.org/abs/2506.09042v3) · [GitHub](https://github.com/nv-tlabs/Cosmos-Drive-Dreams) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Seedance 1.0: Exploring the Boundaries of Video Generation Models | 2025 | [Paper](https://arxiv.org/abs/2506.09113v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval | 2025 | [Paper](https://arxiv.org/abs/2506.08887v1) · [GitHub](https://github.com/LunarShen/DsicoVLA) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation | 2025 | [Paper](https://arxiv.org/abs/2506.08797v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance | 2025 | [Paper](https://arxiv.org/abs/2506.08456v1) · [GitHub](https://github.com/choi403/ALG) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2506.08009v2) · [GitHub](https://github.com/guandeh17/Self-Forcing) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Consistent Video Editing as Flow-Driven Image-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.07713v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.07280v2) · [GitHub](https://github.com/PabloAcuaviva/Gen2Gen) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Restereo: Diffusion stereo video generation and restoration | 2025 | [Paper](https://arxiv.org/abs/2506.06023v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning | 2025 | [Paper](https://arxiv.org/abs/2506.05207v2) · [Website](https://follow-your-motion.github.io/) |
