# arXiv Papers

This page contains arXiv papers related to video generation post-training.

---

| **Title** | **Year** | **Links** |
| --- | --- | :---: |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation | 2025 | [Paper](https://arxiv.org/abs/2512.03621v1) · [GitHub](https://github.com/Iron-LYK/ReCamDriving) · [Website](https://recamdriving.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) In-Context Sync-LoRA for Portrait Video Editing | 2025 | [Paper](https://arxiv.org/abs/2512.03013v1) · [Website](https://sagipolaczek.github.io/Sync-LoRA/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Progressive Image Restoration via Text-Conditioned Video Generation | 2025 | [Paper](https://arxiv.org/abs/2512.02273v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Generative Video Motion Editing with 3D Point Tracks | 2025 | [Paper](https://arxiv.org/abs/2512.02015v1) · [Website](https://edit-by-track.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation | 2025 | [Paper](https://arxiv.org/abs/2512.01960v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models | 2025 | [Paper](https://arxiv.org/abs/2512.01686v1) · [Website](https://yj7082126.github.io/dreamingcomics/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Low-Bitrate Video Compression through Semantic-Conditioned Diffusion | 2025 | [Paper](https://arxiv.org/abs/2512.00408v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement | 2025 | [Paper](https://arxiv.org/abs/2511.23475v1) · [Website](https://hkust-c4g.github.io/AnyTalker-homepage/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.22973v1) · [Website](https://ziplab.co/BlockVid) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) WorldWander: Bridging Egocentric and Exocentric Worlds in Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.22098v1) · [GitHub](https://github.com/showlab/WorldWander) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2511.21129v1) · [Website](https://tele-ai.github.io/CtrlVDiff/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MotionV2V: Editing Motion in a Video | 2025 | [Paper](https://arxiv.org/abs/2511.20640v1) · [Website](https://ryanndagreat.github.io/MotionV2V/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis | 2025 | [Paper](https://arxiv.org/abs/2511.20186v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation | 2025 | [Paper](https://arxiv.org/abs/2511.19320v1) · [Website](https://mcg-nju.github.io/steadydancer-web/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.19049v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) View-Consistent Diffusion Representations for 3D-Consistent Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.18991v1) · [Website](https://danier97.github.io/ViCoDR/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Eevee: Towards Close-up High-resolution Video-based Virtual Try-on | 2025 | [Paper](https://arxiv.org/abs/2511.18957v1) · [GitHub](https://github.com/AMAP-ML/Eevee) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution | 2025 | [Paper](https://arxiv.org/abs/2511.18786v1) · [Website](https://jychen9811.github.io/STCDiT_page/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Point-to-Point: Sparse Motion Guidance for Controllable Video Editing | 2025 | [Paper](https://arxiv.org/abs/2511.18277v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) CamC2V: Context-aware Controllable Video Generation | 2025 | [Paper](https://arxiv.org/abs/2504.06022v2) · [GitHub](https://github.com/LDenninger/CamC2V) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MultiShotMaster: A Controllable Multi-Shot Video Generation Framework | 2025 | [Paper](https://arxiv.org/abs/2512.03041v1) · [GitHub](https://qinghew.github.io/MultiShotMaster/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization | 2025 | [Paper](https://arxiv.org/abs/2512.02933v2) · [GitHub](https://cz-5f.github.io/LoVoRA.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Taming Camera-Controlled Video Generation with Verifiable Geometry Reward | 2025 | [Paper](https://arxiv.org/abs/2512.02870v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) IC-World: In-Context Generation for Shared World Modeling | 2025 | [Paper](https://arxiv.org/abs/2512.02793v1) · [GitHub](https://github.com/wufan-cse/IC-World) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Make Objects Move Slower Than in Reality | 2025 | [Paper](https://arxiv.org/abs/2512.02016v1) · [GitHub](https://gravity-eval.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards | 2025 | [Paper](https://arxiv.org/abs/2512.00425v1) · [GitHub](https://cvlab-stonybrook.github.io/NewtonRewards/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) InstanceV: Instance-Level Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.23146v1) · [GitHub](https://aliothchen.github.io/projects/InstanceV/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning | 2025 | [Paper](https://arxiv.org/abs/2511.22974v1) · [GitHub](https://github.com/QiushiYang/McSc) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) One-to-All Animation: Alignment-Free Character Animation and Image Pose Transfer | 2025 | [Paper](https://arxiv.org/abs/2511.22940v2) · [GitHub](https://ssj9596.github.io/one-to-all-animation-project/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training | 2025 | [Paper](https://arxiv.org/abs/2511.21592v1) · [GitHub](https://xavihart.github.io/mogan/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Video Generation Models Are Good Latent Reward Models | 2025 | [Paper](https://arxiv.org/abs/2511.21541v1) · [GitHub](https://kululumi.github.io/PRFL/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models | 2025 | [Paper](https://arxiv.org/abs/2511.20629v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Growing with the Generator: Self-paced GRPO for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.19356v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation | 2025 | [Paper](https://arxiv.org/abs/2511.18919v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses | 2025 | [Paper](https://arxiv.org/abs/2511.18173v1) · [Website](https://cvg-bonn.github.io/EgoControl/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.17844v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Show Me: Unifying Instructional Image and Video Generation with Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2511.17839v1) · [GitHub](https://yujiangpu20.github.io/showme/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO | 2025 | [Paper](https://arxiv.org/abs/2511.16669v2) · [GitHub](https://video-as-answer.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) First Frame Is the Place to Go for Video Content Customization | 2025 | [Paper](https://arxiv.org/abs/2511.15700v1) · [Website](http://firstframego.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.14993v2) · [GitHub](https://github.com/kandinskylab/kandinsky-5) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models | 2025 | [Paper](https://arxiv.org/abs/2511.13704v1) · [GitHub](https://haroldchen19.github.io/TiViBench-Page/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2511.12099v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.12072v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LiteAttention: A Temporal Sparse Attention for Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2511.11062v1) · [GitHub](https://github.com/moonmath-ai/LiteAttention) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation | 2025 | [Paper](https://arxiv.org/abs/2511.11002v1) · [Website](https://zane-zyqiu.github.io/EmoVid) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Video Text Preservation with Synthetic Text-Rich Videos | 2025 | [Paper](https://arxiv.org/abs/2511.05573v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Neodragon: Mobile Video Generation using Diffusion Transformer | 2025 | [Paper](https://arxiv.org/abs/2511.06055v1) · [GitHub](https://qualcomm-ai-research.github.io/neodragon) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.04317v1) · [GitHub](https://rise-t2v.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection | 2025 | [Paper](https://arxiv.org/abs/2511.03997v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions | 2025 | [Paper](https://arxiv.org/abs/2511.03334v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ID-Crafter: VLM-Grounded Online RL for Compositional Multi-Subject Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.00511v3) · [GitHub](https://angericky.github.io/ID-Crafter/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V | 2025 | [Paper](https://arxiv.org/abs/2510.27364v1) · [GitHub](https://sedatbvb5.github.io/AyDNA/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) The Quest for Generalizable Motion Generation: Data, Model, and Evaluation | 2025 | [Paper](https://arxiv.org/abs/2510.26794v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) VC4VG: Optimizing Video Captions for Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2510.24134) · [GitHub](https://github.com/qyr0403/VC4VG) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) CoMo: Compositional Motion Customization for Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2510.23007v1) · [GitHub](https://como6.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Epipolar Geometry Improves Video Generation Models | 2025 | [Paper](https://arxiv.org/abs/2510.21615v1) · [GitHub](https://epipolar-dpo.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2510.19022v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) TGT: Text-Grounded Trajectories for Locally Controlled Video Generation | 2025 | [Paper](https://arxiv.org/abs/2510.15104v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) In-Context Learning with Unpaired Clips for Instruction-based Video Editing | 2025 | [Paper](https://arxiv.org/abs/2510.14648v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning | 2025 | [Paper](https://arxiv.org/abs/2510.14256v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization | 2025 | [Paper](https://arxiv.org/abs/2510.14255v3) · [GitHub](https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/Reward_FL/) · [Website](https://ipro-alimama.github.io/)|
| ![arXiv](https://img.shields.io/badge/arXiv-red) Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures | 2025 | [Paper](https://arxiv.org/abs/2510.14179v1) · [Website](https://eyeline-labs.github.io/Virtually-Being/)|
| ![arXiv](https://img.shields.io/badge/arXiv-red) PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning | 2025 | [Paper](https://arxiv.org/abs/2510.13809v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars | 2025 | [Paper](https://arxiv.org/abs/2510.12785v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Stable Video Infinity: Infinite-Length Video Generation with Error Recycling | 2025 | [Paper](https://arxiv.org/abs/2510.09212v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency | 2025 | [Paper](https://arxiv.org/abs/2510.08431v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Real-Time Motion-Controllable Autoregressive Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2510.08131v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PickStyle: Video-to-Video Style Transfer with Context-Style Adapters | 2025 | [Paper](https://arxiv.org/abs/2510.07546v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Mitigating Surgical Data Imbalance with Dual-Prediction Video Diffusion Model | 2025 | [Paper](https://arxiv.org/abs/2510.07345v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MATRIX: Mask Track Alignment for Interaction-aware Video Generation | 2025 | [Paper](https://arxiv.org/abs/2510.07310v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report | 2025 | [Paper](https://arxiv.org/abs/2510.07092v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Character Mixing for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2510.05093v1) · [GitHub](https://github.com/TingtingLiao/mimix) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Arbitrary Generative Video Interpolation | 2025 | [Paper](https://arxiv.org/abs/2510.00578v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2509.25187v2) · [GitHub](https://pku-yuangroup.github.io/FlashI2V) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder | 2025 | [Paper](https://arxiv.org/abs/2509.25182v1) · [GitHub](https://github.com/dc-ai-projects/DC-VideoGen) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Rolling Forcing: Autoregressive Long Video Diffusion in Real Time | 2025 | [Paper](https://arxiv.org/abs/2509.25161v1) · [GitHub](https://kunhao-liu.github.io/Rolling_Forcing_Webpage) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2509.24997v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer | 2025 | [Paper](https://arxiv.org/abs/2509.24899v3) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Reinforcement Learning with Inverse Rewards for World Model Post-training | 2025 | [Paper](https://arxiv.org/abs/2509.23958v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models | 2025 | [Paper](https://arxiv.org/abs/2509.21760v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Echo-Path: Pathology-Conditioned Echo Video Generation | 2025 | [Paper](https://arxiv.org/abs/2509.17190v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) VidCLearn: A Continual Learning Approach for Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2509.16956v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Lynx: Towards High-Fidelity Personalized Video Generation | 2025 | [Paper](https://arxiv.org/abs/2509.15496v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data | 2025 | [Paper](https://arxiv.org/abs/2509.15479v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation | 2025 | [Paper](https://arxiv.org/abs/2509.11092v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis | 2025 | [Paper](https://arxiv.org/abs/2509.09595v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders | 2025 | [Paper](https://arxiv.org/abs/2509.09547v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) RewardDance: Reward Scaling in Visual Generation | 2025 | [Paper](https://arxiv.org/abs/2509.08826v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning | 2025 | [Paper](https://arxiv.org/abs/2509.08519v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training | 2025 | [Paper](https://arxiv.org/abs/2509.06723v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) UniVerse-1: Unified Audio-Video Generation via Stitching of Experts | 2025 | [Paper](https://arxiv.org/abs/2509.06155v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2509.06040v5) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview | 2025 | [Paper](https://arxiv.org/abs/2509.04450v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective | 2025 | [Paper](https://arxiv.org/abs/2509.00403v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.20471v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Learning Primitive Embodied World Models: Towards Scalable Robotic Learning | 2025 | [Paper](https://arxiv.org/abs/2508.20840v3) · [GitHub](https://github.com/qiaosun22/PrimitiveWorld) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Phased One-Step Adversarial Equilibrium for Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2508.21019v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.19320v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ControlEchoSynth: Boosting Ejection Fraction Estimation Models via Controlled Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2508.17631v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MoSA: Motion-Coherent Human Video Generation via Structure-Appearance Decoupling | 2025 | [Paper](https://arxiv.org/abs/2508.17404v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.17062v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation | 2025 | [Paper](https://arxiv.org/abs/2508.16512v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception | 2025 | [Paper](https://arxiv.org/abs/2508.15720v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration | 2025 | [Paper](https://arxiv.org/abs/2508.14483v3) · [GitHub](https://github.com/csbhr/Vivid-VR) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing | 2025 | [Paper](https://arxiv.org/abs/2508.14465v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) 4DNeX: Feed-Forward 4D Generative Modeling Made Easy | 2025 | [Paper](https://arxiv.org/abs/2508.13154v1) · [GitHub](https://github.com/3DTopia/4DNeX) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Physical Autoregressive Model for Robotic Manipulation without Action Pretraining | 2025 | [Paper](https://arxiv.org/abs/2508.09822v4) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts | 2025 | [Paper](https://arxiv.org/abs/2508.09476v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Animate-X++: Universal Character Image Animation with Dynamic Backgrounds | 2025 | [Paper](https://arxiv.org/abs/2508.09454v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space | 2025 | [Paper](https://arxiv.org/abs/2508.08588v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation | 2025 | [Paper](https://arxiv.org/abs/2508.07981v3) · [Website](https://amap-ml.github.io/Omni-Effects.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2508.07149v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment | 2025 | [Paper](https://arxiv.org/abs/2508.06082v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DreamVE: Unified Instruction-based Image and Video Editing | 2025 | [Paper](https://arxiv.org/abs/2508.06080v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.05091v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.04228v1) · [Website](https://kr-panghu.github.io/LayerT2V/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Scaling Up Audio-Synchronized Visual Animation: An Efficient Training Paradigm | 2025 | [Paper](https://arxiv.org/abs/2508.03955v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.03694v1) · [Website](https://vchitect.github.io/LongVie-project/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.03334v3) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework | 2025 | [Paper](https://arxiv.org/abs/2508.02807v1) · [Website](https://virtu-lab.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Compositional Video Synthesis by Temporal Object-Centric Learning | 2025 | [Paper](https://arxiv.org/abs/2507.20855v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) AnimeColor: Reference-based Animation Colorization with Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2507.20158v1) · [GitHub](https://github.com/IamCreateAI/AnimeColor) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Enhancing Scene Transition Awareness in Video Generation via Post-Training | 2025 | [Paper](https://arxiv.org/abs/2507.18046v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA | 2025 | [Paper](https://arxiv.org/abs/2507.17963v1) · [Website](https://snap-research.github.io/zero-shot-dynamic-concepts/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with Video Diffusion Model | 2025 | [Paper](https://arxiv.org/abs/2507.16341v1) · [GitHub](https://github.com/MingtaoGuo/Face-Reenactment-Video-Diffusion) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation | 2025 | [Paper](https://arxiv.org/abs/2507.16116v1) · [Website](https://yaofang-liu.github.io/Pusa_Web/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Conditional Video Generation for High-Efficiency Video Compression | 2025 | [Paper](https://arxiv.org/abs/2507.15269v4) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation | 2025 | [Paper](https://arxiv.org/abs/2507.15064v1) · [Website](https://francis-rings.github.io/StableAnimator/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2507.13344v1) · [Website](https://diffuman4d.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling | 2025 | [Paper](https://arxiv.org/abs/2507.07982v1) · [Website](https://GeometryForcing.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Geometry-aware 4D Video Generation for Robot Manipulation | 2025 | [Paper](https://arxiv.org/abs/2507.01099v1) · [GitHub](https://github.com/lzylucy/4dgen) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Populate-A-Scene: Affordance-Aware Human Video Generation | 2025 | [Paper](https://arxiv.org/abs/2507.00334v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) TextMesh4D: High-Quality Text-to-4D Mesh Generation | 2025 | [Paper](https://arxiv.org/abs/2506.24121v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.23690v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis | 2025 | [Paper](https://arxiv.org/abs/2506.23263v1) · [Website](http://lotvsmmau.net/Causal-VidSyn) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) RoboScape: Physics-informed Embodied World Model | 2025 | [Paper](https://arxiv.org/abs/2506.23135v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy | 2025 | [Paper](https://arxiv.org/abs/2506.22432v2) · [Website](https://shapeformotion.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation | 2025 | [Paper](https://arxiv.org/abs/2506.22065v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) FairyGen: Storied Cartoon Video from a Single Child-Drawn Character | 2025 | [Paper](https://arxiv.org/abs/2506.21272v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Video Virtual Try-on with Conditional Diffusion Transformer Inpainter | 2025 | [Paper](https://arxiv.org/abs/2506.21270v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing | 2025 | [Paper](https://arxiv.org/abs/2506.20967v2) · [Website](https://dfvedit.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Bind-Your-Avatar: Multi-Talking-Character Video Generation with Dynamic 3D-mask-based Embedding Router | 2025 | [Paper](https://arxiv.org/abs/2506.19833v1) · [GitHub](https://github.com/Yubo-Shankui/Bind-Your-Avatar-Implementation) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.19851v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) RDPO: Real Data Preference Optimization for Physics Consistency Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.18655v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) FramePrompt: In-context Controllable Animation with Zero Structural Changes | 2025 | [Paper](https://arxiv.org/abs/2506.17301v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning | 2025 | [Paper](https://arxiv.org/abs/2506.14827v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Causally Steered Diffusion for Automated Video Counterfactual Generation | 2025 | [Paper](https://arxiv.org/abs/2506.14404v2) · [GitHub](https://github.com/nysp78/counterfactual-video-generation) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer | 2025 | [Paper](https://arxiv.org/abs/2506.12847v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PlayerOne: Egocentric World Simulator | 2025 | [Paper](https://arxiv.org/abs/2506.09995v1) · [GitHub](https://github.com/yuanpengtu/PlayerOne) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning | 2025 | [Paper](https://arxiv.org/abs/2506.10082v5) · [GitHub](https://github.com/cjeen/LoRAEdit) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2506.10568v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning | 2025 | [Paper](https://arxiv.org/abs/2506.10639v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation | 2025 | [Paper](https://arxiv.org/abs/2506.11144v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance | 2025 | [Paper](https://arxiv.org/abs/2506.08456v1) · [GitHub](https://github.com/choi403/ALG) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation | 2025 | [Paper](https://arxiv.org/abs/2506.08797v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval | 2025 | [Paper](https://arxiv.org/abs/2506.08887v1) · [GitHub](https://github.com/LunarShen/DsicoVLA) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Seedance 1.0: Exploring the Boundaries of Video Generation Models | 2025 | [Paper](https://arxiv.org/abs/2506.09113v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models | 2025 | [Paper](https://arxiv.org/abs/2506.09042v3) · [GitHub](https://github.com/nv-tlabs/Cosmos-Drive-Dreams) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.09229v2) · [GitHub](https://github.com/deepshwang/crepa) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2506.08009v2) · [GitHub](https://github.com/guandeh17/Self-Forcing) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Consistent Video Editing as Flow-Driven Image-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.07713v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.07280v2) · [GitHub](https://github.com/PabloAcuaviva/Gen2Gen) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Restereo: Diffusion stereo video generation and restoration | 2025 | [Paper](https://arxiv.org/abs/2506.06023v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning | 2025 | [Paper](https://arxiv.org/abs/2506.05207v2) · [Website](https://follow-your-motion.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Follow-Your-Creation: Empowering 4D Creation through Video Inpainting | 2025 | [Paper](https://arxiv.org/abs/2506.04590v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) UNIC: Unified In-Context Video Editing | 2025 | [Paper](https://arxiv.org/abs/2506.04216v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2506.04213v2) · [Website](https://fulldit2.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) CamCloneMaster: Enabling Reference-based Camera Control for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.03140v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Controllable Human-centric Keyframe Interpolation with Generative Prior | 2025 | [Paper](https://arxiv.org/abs/2506.03119v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ORV: 4D Occupancy-centric Robot Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.03079v2) · [Website](https://orangesodahub.github.io/ORV) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2506.03065v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering | 2025 | [Paper](https://arxiv.org/abs/2506.02733v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Physics-Guided Motion Loss for Video Generation Model | 2025 | [Paper](https://arxiv.org/abs/2506.02244v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control | 2025 | [Paper](https://arxiv.org/abs/2506.01943v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation | 2025 | [Paper](https://arxiv.org/abs/2506.01801v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks | 2025 | [Paper](https://arxiv.org/abs/2506.01758v2) · [GitHub](https://github.com/leeruibin/MfM.git) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents | 2025 | [Paper](https://arxiv.org/abs/2506.01689v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation | 2025 | [Paper](https://arxiv.org/abs/2506.01591v1) · [GitHub](https://github.com/yuangan/Silencer) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Playing with Transformer at 30+ FPS via Next-Frame Diffusion | 2025 | [Paper](https://arxiv.org/abs/2506.01380v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DeepVerse: 4D Autoregressive Video Generation as a World Model | 2025 | [Paper](https://arxiv.org/abs/2506.01103v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.00996v1) · [Website](https://kinam0252.github.io/TIC-FT/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2506.00830v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Video Signature: Implicit Watermarking for Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.00652v4) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes | 2025 | [Paper](https://arxiv.org/abs/2506.00227v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MAGREF: Masked Guidance for Any-Reference Video Generation with Subject Disentanglement | 2025 | [Paper](https://arxiv.org/abs/2505.23742v2) · [GitHub](https://github.com/MAGREF-Video/MAGREF) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ATI: Any Trajectory Instruction for Controllable Video Generation | 2025 | [Paper](https://arxiv.org/abs/2505.22944v3) · [Website](https://anytraj.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) FaceEditTalker: Controllable Talking Head Generation with Facial Attribute Editing | 2025 | [Paper](https://arxiv.org/abs/2505.22141v2) · [Website](https://peterfanfan.github.io/FaceEditTalker/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance | 2025 | [Paper](https://arxiv.org/abs/2505.21876v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Think Before You Diffuse: Infusing Physical Rules into Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2505.21653v3) · [Website](https://bwgzk-keke.github.io/DiffPhy/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Any-to-Bokeh: Arbitrary-Subject Video Refocusing with Video Diffusion Model | 2025 | [Paper](https://arxiv.org/abs/2505.21593v3) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MotionPro: A Precise Motion Controller for Image-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2505.20287v1) · [Website](https://zhw-zhang.github.io/MotionPro-page/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters | 2025 | [Paper](https://arxiv.org/abs/2505.20156v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM | 2025 | [Paper](https://arxiv.org/abs/2505.19901v3) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation | 2025 | [Paper](https://arxiv.org/abs/2505.18078v1) · [Website](https://DanceTog.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain | 2025 | [Paper](https://arxiv.org/abs/2505.17727v1) · [Website](https://zhoujiawei3.github.io/SafeMVDrive/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2505.17550v3) · [GitHub](https://github.com/VDIGPKU/T2VUnlearning.git) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance | 2025 | [Paper](https://arxiv.org/abs/2505.13437v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation | 2025 | [Paper](https://arxiv.org/abs/2505.10238v4) · [GitHub](https://github.com/DINGYANB/MTVCrafter) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2505.09858v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model | 2025 | [Paper](https://arxiv.org/abs/2505.07449v7) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images | 2025 | [Paper](https://arxiv.org/abs/2505.06537v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation | 2025 | [Paper](https://arxiv.org/abs/2505.04512v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) A Unit Enhancement and Guidance Framework for Audio-Driven Avatar Video Generation | 2025 | [Paper](https://arxiv.org/abs/2505.03603v5) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2505.01406v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction | 2025 | [Paper](https://arxiv.org/abs/2504.21855v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation | 2025 | [Paper](https://arxiv.org/abs/2504.21650v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance | 2025 | [Paper](https://arxiv.org/abs/2504.16464v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Subject-driven Video Generation via Disentangled Identity and Motion | 2025 | [Paper](https://arxiv.org/abs/2504.17816v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning | 2025 | [Paper](https://arxiv.org/abs/2504.15932v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion Transformer | 2025 | [Paper](https://arxiv.org/abs/2504.11289v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Taming Consistency Distillation for Accelerated Human Image Animation | 2025 | [Paper](https://arxiv.org/abs/2504.11143v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Aligning Anime Video Generation with Human Feedback | 2025 | [Paper](https://arxiv.org/abs/2504.10044v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) I Want It That Way! Specifying Nuanced Camera Motions in Video Editing | 2025 | [Paper](https://arxiv.org/abs/2504.09472v2) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Discriminator-Free Direct Preference Optimization for Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2504.08542v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) EasyGenNet: An Efficient Framework for Audio-Driven Gesture Video Generation Based on Diffusion Model | 2025 | [Paper](https://arxiv.org/abs/2504.08344v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) RAGME: Retrieval Augmented Video Generation for Enhanced Motion Realism | 2025 | [Paper](https://arxiv.org/abs/2504.06672v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation | 2023 | [Paper](https://arxiv.org/abs/2305.10874) |



