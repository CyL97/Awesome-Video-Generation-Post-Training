<div align=center>

# Awesome Video Generation Post Training

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)
[![arXiv](https://img.shields.io/badge/arXiv-xxxx\.xxxxx-red.svg)]()

[[arXiv]]() [[HuggingFace]]() [[Database]]()

</div>

> **** [[arXiv]]()  
> [Chaoyu Li](https://chaoyuli.com/)<sup>\*,1,‚Ä†</sup>,
> Xiaoyi Gu<sup>2,‚Ä†</sup>,
> Yogesh Kulkarni<sup>1</sup>,
> Eun Woo Im<sup>1</sup>,
> Mohammadmahdi Honarmand<sup>3</sup>,
> Zeyu Wang<sup>4</sup>,
> Juntong Song<sup>5</sup>,
> Fei Du<sup>6</sup>,
> Xilin Jiang<sup>7</sup>,
> Kexin Zheng<sup>8</sup>,
> Tianzhi Li<sup>9</sup>,
> Fei Tao<sup>5</sup>,
> Pooyan Fazli<sup>1</sup>  
>
> <sup>1</sup>Arizona State University ¬∑
> <sup>2</sup>Twitch ¬∑
> <sup>3</sup>Stanford University ¬∑
> <sup>4</sup>eBay ¬∑
> <sup>5</sup>NewsBreak ¬∑
> <sup>6</sup>Microsoft ¬∑
> <sup>7</sup>Columbia University ¬∑
> <sup>8</sup>University of Southern California ¬∑
> <sup>9</sup>Carnegie Mellon University  
>
> <sup>‚Ä†</sup>Equal contribution. <sup>\*</sup>Corresponding author: Chaoyu Li (chaoyuli@asu.edu).

---

> [!IMPORTANT]
> We welcome your help in improving the repository and paper. Please feel free to submit a [pull request]() to:
> 
> - Add a relevant paper not yet included.
>
> - Suggest a more suitable category.
>
> - Update the information.
>
> - Ask for clarification about any content.

---

## üî• News

- **[2026.xx.xx]** The v1 survey is now published! We've also initialized the repository.

## üéØ Motivation

> **Motivation:** .

## üìå Citation

If you find our paper or this resource helpful, please consider cite:

```bibtex
@article{,
  title={},
  author={},
  journal={arXiv preprint arXiv:xxxx.xxxxx},
  year={2026}
}
```

## üìö Contents

- [Awesome Video Generation Post Training](#)
    - [Arxiv Paper](https://github.com/.md)
    - [Conference Paper](conference.md)

---

### (WIP) Paper Published in Arxiv

| **Title (Conference / Journal)** | **Year** | **Paper** |
| --- | --- | :---: |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) CamC2V: Context-aware Controllable Video Generation | 2025 | [Paper](https://arxiv.org/abs/2504.06022v2) ¬∑ [GitHub](https://github.com/LDenninger/CamC2V) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) MultiShotMaster: A Controllable Multi-Shot Video Generation Framework | 2025 | [Paper](https://arxiv.org/abs/2512.03041v1) ¬∑ [GitHub](https://qinghew.github.io/MultiShotMaster/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization | 2025 | [Paper](https://arxiv.org/abs/2512.02933v2) ¬∑ [GitHub](https://cz-5f.github.io/LoVoRA.github.io/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Taming Camera-Controlled Video Generation with Verifiable Geometry Reward | 2025 | [Paper](https://arxiv.org/abs/2512.02870v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) IC-World: In-Context Generation for Shared World Modeling | 2025 | [Paper](https://arxiv.org/abs/2512.02793v1) ¬∑ [GitHub](https://github.com/wufan-cse/IC-World) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Make Objects Move Slower Than in Reality | 2025 | [Paper](https://arxiv.org/abs/2512.02016v1) ¬∑ [GitHub](https://gravity-eval.github.io) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards | 2025 | [Paper](https://arxiv.org/abs/2512.00425v1) ¬∑ [GitHub](https://cvlab-stonybrook.github.io/NewtonRewards/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) InstanceV: Instance-Level Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.23146v1) ¬∑ [GitHub](https://aliothchen.github.io/projects/InstanceV/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning | 2025 | [Paper](https://arxiv.org/abs/2511.22974v1) ¬∑ [GitHub](https://github.com/QiushiYang/McSc) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) One-to-All Animation: Alignment-Free Character Animation and Image Pose Transfer | 2025 | [Paper](https://arxiv.org/abs/2511.22940v2) ¬∑ [GitHub](https://ssj9596.github.io/one-to-all-animation-project/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training | 2025 | [Paper](https://arxiv.org/abs/2511.21592v1) ¬∑ [GitHub](https://xavihart.github.io/mogan/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Video Generation Models Are Good Latent Reward Models | 2025 | [Paper](https://arxiv.org/abs/2511.21541v1) ¬∑ [GitHub](https://kululumi.github.io/PRFL/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses | 2025 | [Paper](https://arxiv.org/abs/2511.18173v1) ¬∑ [Website](https://cvg-bonn.github.io/EgoControl/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.17844v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Show Me: Unifying Instructional Image and Video Generation with Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2511.17839v1) ¬∑ [GitHub](https://yujiangpu20.github.io/showme/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO | 2025 | [Paper](https://arxiv.org/abs/2511.16669v2) ¬∑ [GitHub](https://video-as-answer.github.io) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) First Frame Is the Place to Go for Video Content Customization | 2025 | [Paper](https://arxiv.org/abs/2511.15700v1) ¬∑ [Website](http://firstframego.github.io) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.14993v2) ¬∑ [GitHub](https://github.com/kandinskylab/kandinsky-5) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models | 2025 | [Paper](https://arxiv.org/abs/2511.13704v1) ¬∑ [GitHub](https://haroldchen19.github.io/TiViBench-Page/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2511.12099v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.12072v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation | 2025 | [Paper](https://arxiv.org/abs/2511.11002v1) ¬∑ [Website](https://zane-zyqiu.github.io/EmoVid) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) LiteAttention: A Temporal Sparse Attention for Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2511.11062v1) ¬∑ [GitHub](https://github.com/moonmath-ai/LiteAttention) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Neodragon: Mobile Video Generation using Diffusion Transformer | 2025 | [Paper](https://arxiv.org/abs/2511.06055v1) ¬∑ [GitHub](https://qualcomm-ai-research.github.io/neodragon) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Video Text Preservation with Synthetic Text-Rich Videos | 2025 | [Paper](https://arxiv.org/abs/2511.05573v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.04317v1) ¬∑ [GitHub](https://rise-t2v.github.io) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions | 2025 | [Paper](https://arxiv.org/abs/2511.03334v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection | 2025 | [Paper](https://arxiv.org/abs/2511.03997v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) VC4VG: Optimizing Video Captions for Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2510.24134) ¬∑ [GitHub](https://github.com/qyr0403/VC4VG) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) ID-Crafter: VLM-Grounded Online RL for Compositional Multi-Subject Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.00511v3) ¬∑ [GitHub](https://angericky.github.io/ID-Crafter/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V | 2025 | [Paper](https://arxiv.org/abs/2510.27364v1) ¬∑ [GitHub](https://sedatbvb5.github.io/AyDNA/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Epipolar Geometry Improves Video Generation Models | 2025 | [Paper](https://arxiv.org/abs/2510.21615v1) ¬∑ [GitHub](https://epipolar-dpo.github.io) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2510.19022v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) PickStyle: Video-to-Video Style Transfer with Context-Style Adapters | 2025 | [Paper](https://arxiv.org/abs/2510.07546v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) MATRIX: Mask Track Alignment for Interaction-aware Video Generation | 2025 | [Paper](https://arxiv.org/abs/2510.07310v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Mitigating Surgical Data Imbalance with Dual-Prediction Video Diffusion Model | 2025 | [Paper](https://arxiv.org/abs/2510.07345v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report | 2025 | [Paper](https://arxiv.org/abs/2510.07092v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Character Mixing for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2510.05093v1) ¬∑ [GitHub](https://github.com/TingtingLiao/mimix) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Arbitrary Generative Video Interpolation | 2025 | [Paper](https://arxiv.org/abs/2510.00578v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2509.25187v2) ¬∑ [GitHub](https://pku-yuangroup.github.io/FlashI2V) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder | 2025 | [Paper](https://arxiv.org/abs/2509.25182v1) ¬∑ [GitHub](https://github.com/dc-ai-projects/DC-VideoGen) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Rolling Forcing: Autoregressive Long Video Diffusion in Real Time | 2025 | [Paper](https://arxiv.org/abs/2509.25161v1) ¬∑ [GitHub](https://kunhao-liu.github.io/Rolling_Forcing_Webpage) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2509.24997v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer | 2025 | [Paper](https://arxiv.org/abs/2509.24899v3) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Reinforcement Learning with Inverse Rewards for World Model Post-training | 2025 | [Paper](https://arxiv.org/abs/2509.23958v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2509.06040v5) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview | 2025 | [Paper](https://arxiv.org/abs/2509.04450v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective | 2025 | [Paper](https://arxiv.org/abs/2509.00403v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Phased One-Step Adversarial Equilibrium for Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2508.21019v2) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Learning Primitive Embodied World Models: Towards Scalable Robotic Learning | 2025 | [Paper](https://arxiv.org/abs/2508.20840v3) ¬∑ [GitHub](https://github.com/qiaosun22/PrimitiveWorld) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.20471v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.19320v2) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) ControlEchoSynth: Boosting Ejection Fraction Estimation Models via Controlled Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2508.17631v2) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) MoSA: Motion-Coherent Human Video Generation via Structure-Appearance Decoupling | 2025 | [Paper](https://arxiv.org/abs/2508.17404v2) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.17062v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation | 2025 | [Paper](https://arxiv.org/abs/2508.16512v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration | 2025 | [Paper](https://arxiv.org/abs/2508.14483v3) ¬∑ [GitHub](https://github.com/csbhr/Vivid-VR) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing | 2025 | [Paper](https://arxiv.org/abs/2508.14465v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) 4DNeX: Feed-Forward 4D Generative Modeling Made Easy | 2025 | [Paper](https://arxiv.org/abs/2508.13154v1) ¬∑ [GitHub](https://github.com/3DTopia/4DNeX) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Physical Autoregressive Model for Robotic Manipulation without Action Pretraining | 2025 | [Paper](https://arxiv.org/abs/2508.09822v4) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts | 2025 | [Paper](https://arxiv.org/abs/2508.09476v2) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception | 2025 | [Paper](https://arxiv.org/abs/2508.15720v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models | 2025 | [Paper](https://arxiv.org/abs/2511.20629v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Growing with the Generator: Self-paced GRPO for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.19356v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation | 2025 | [Paper](https://arxiv.org/abs/2511.18919v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models | 2025 | [Paper](https://arxiv.org/abs/2511.13704v1) ¬∑ [GitHub](https://haroldchen19.github.io/TiViBench-Page/) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) LiteAttention: A Temporal Sparse Attention for Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2511.11062v1) ¬∑ [GitHub](https://github.com/moonmath-ai/LiteAttention) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) Neodragon: Mobile Video Generation using Diffusion Transformer | 2025 | [Paper](https://arxiv.org/abs/2511.06055v1) ¬∑ [GitHub](https://qualcomm-ai-research.github.io/neodragon) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.04317v1) ¬∑ [GitHub](https://rise-t2v.github.io) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection | 2025 | [Paper](https://arxiv.org/abs/2511.03997v1) |
| ![arXiv Badge](https://img.shields.io/badge/arXiv-red) The Quest for Generalizable Motion Generation: Data, Model, and Evaluation | 2025 | [Paper](https://arxiv.org/abs/2510.26794v1) |


---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üßë‚Äçüíª Contributors

üëè Thanks to these contributors for this excellent workÔºÅ

<a href="https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=cokeshao/Awesome-Multimodal-Token-Compression" />
</a>

## ‚úâÔ∏è Contact

For questions, suggestions, or collaboration opportunities, please feel free to reach out:

‚úâÔ∏è Email:  [chaoyuli@asu.edu](mailto:chaoyuli@asu.edu)

## ‚ú® Star History

[![Star History Chart](https://api.star-history.com/svg?repos=CyL97/Awesome-Video-Generation-Post-Training&type=date&legend=top-left)](https://www.star-history.com/#CyL97/Awesome-Video-Generation-Post-Training&type=date&legend=top-left)
