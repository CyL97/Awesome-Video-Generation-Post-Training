<div align=center>

# Awesome Video Generation Post Training

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)
[![arXiv](https://img.shields.io/badge/arXiv-xxxx\.xxxxx-red.svg)]()

[[arXiv]]() [[HuggingFace]]() [[Database]]()

</div>

> **** [[arXiv]]()  
> [Chaoyu Li](https://chaoyuli.com/)<sup>\*,1,‚Ä†</sup>,
> Xiaoyi Gu<sup>2,‚Ä†</sup>,
> Yogesh Kulkarni<sup>1</sup>,
> Eun Woo Im<sup>1</sup>,
> Mohammadmahdi Honarmand<sup>3</sup>,
> Zeyu Wang<sup>4</sup>,
> Juntong Song<sup>5</sup>,
> Fei Du<sup>6</sup>,
> Xilin Jiang<sup>7</sup>,
> Kexin Zheng<sup>8</sup>,
> Tianzhi Li<sup>9</sup>,
> Fei Tao<sup>5</sup>,
> Pooyan Fazli<sup>1</sup>  
>
> <sup>1</sup>Arizona State University ¬∑
> <sup>2</sup>Twitch ¬∑
> <sup>3</sup>Stanford University ¬∑
> <sup>4</sup>eBay ¬∑
> <sup>5</sup>NewsBreak ¬∑
> <sup>6</sup>Microsoft ¬∑
> <sup>7</sup>Columbia University ¬∑
> <sup>8</sup>University of Southern California ¬∑
> <sup>9</sup>Carnegie Mellon University  
>
> <sup>‚Ä†</sup>Equal contribution. <sup>\*</sup>Corresponding author: Chaoyu Li (chaoyuli@asu.edu).

---

> [!IMPORTANT]
> We welcome your help in improving the repository and paper. Please feel free to submit a [pull request]() to:
> 
> - Add a relevant paper not yet included.
>
> - Suggest a more suitable category.
>
> - Update the information.
>
> - Ask for clarification about any content.

---

## üî• News

- **[2026.xx.xx]** The v1 survey is now published! We've also initialized the repository.

## üéØ Motivation

> **Motivation:** .

## üìå Citation

If you find our paper or this resource helpful, please consider cite:

```bibtex
@article{,
  title={},
  author={},
  journal={arXiv preprint arXiv:xxxx.xxxxx},
  year={2026}
}
```

## üìö Contents

- [Awesome Video Generation Post Training](#)
    - [Arxiv Paper](arxiv.md)
    - [Conference Paper](conference.md)

---

### Recent Conference Paper

| **Title (Conference / Journal)** | **Year** | **Paper** |
| --- | --- | :---: |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) PanFlow: Decoupled Motion Control for Panoramic Video Generation | 2026 | [Paper](https://arxiv.org/abs/2512.00832v1) ¬∑ [GitHub](https://github.com/chengzhag/PanFlow) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) CAGE:Unsupervised Visual Composition and Animation for Controllable Video Generation | 2025 | [Paper](https://arxiv.org/abs/2403.14368) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities | 2025 | [Paper](https://arxiv.org/abs/2408.13239) ¬∑ [GitHub](https://github.com/WuTao-CS/CustomCrafter) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) CustomTTT: Motion and Appearance Customized Video Generation via Test-Time Training | 2025 | [Paper](https://arxiv.org/abs/2412.15646) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation | 2025 | [Paper](https://arxiv.org/abs/2412.04563) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) FROG: A Robust and Efficient Probabilistic Framework for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2407.13509) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) Free-Form Motion Control: Controlling the 6D Poses of Camera and Objects in Video Generation | 2025 | [Paper](https://arxiv.org/abs/2501.01425) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) FramePainter: Endowing Video Diffusion Model with Object-Guided Trajectory Control | 2025 | [Paper](https://arxiv.org/abs/2410.15069) ¬∑ [GitHub](https://github.com/yzhangcs/FramePainter) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) I2V-Adapter: Learning Image-to-Video Adaptation from a Single Video Pair | 2025 | [Paper](https://arxiv.org/abs/2312.16693) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) I2VControl: Disentangled and Unified Video Motion Synthesis Control | 2025 | [Paper](https://arxiv.org/abs/2411.17765) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) Implicit Motion Handling for Video Generation: A Self-Training Strategy | 2025 | [Paper](https://arxiv.org/abs/2406.09519) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) Panoramic Video Generation: A Novel Dataset and Improved Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2409.02839) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) Pixel-level Motion Control for Diffusion-based Video Generation | 2025 | [Paper](https://arxiv.org/abs/2408.00981) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) PiSSA: Parameter-Efficient Fine-Tuning with Singular Values | 2025 | [Paper](https://arxiv.org/abs/2402.09353) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) Quantum Reinforcement Learning for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2408.03883) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) RAVE: Retrieval-Augmented Video Generation | 2025 | [Paper](https://arxiv.org/abs/2409.05002) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) StyleCrafter: Style-Driven Diverse and Disentangled Video Generation | 2025 | [Paper](https://arxiv.org/abs/2409.06826) |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) Text2video-Zero: Text-to-video Generation with Zero-shot Multimodal Control | 2025 | [Paper](https://arxiv.org/abs/2409.14490) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Aligning What Matters: Masked Latent Adaptation for Text-to-Audio-Video Generation | 2025 |  |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Audio-Sync Video Generation with Multi-Stream Temporal Control | 2025 | [Paper](https://arxiv.org/abs/2506.08003v1) ¬∑ [GitHub](https://github.com/suimuc/MTV_Framework) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.09350v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.03517v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) EchoShot: Multi-Shot Portrait Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.15838) ¬∑ [GitHub](https://github.com/JoHnneyWang/EchoShot) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Frame Context Packing and Drift Prevention in Next-Frame-Prediction Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2504.12626) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Frame In-N-Out: Unbounded Controllable Image-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2505.21491v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) GeoVideo: Introducing Geometric Regularization into Video Generation Model | 2025 | [Paper](https://arxiv.org/abs/2512.03453v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.10858v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Imagine360: Immersive 360 Video Generation from Perspective Anchor | 2025 | [Paper](https://arxiv.org/abs/2412.03552) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Improving Video Generation with Human Feedback | 2025 | [Paper](https://arxiv.org/abs/2501.13918) ¬∑ [GitHub](https://github.com/KlingTeam/VideoAlign) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) MoCha: Towards Movie-Grade Talking Character Generation | 2025 | [Paper](https://arxiv.org/abs/2503.23307) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement | 2025 | [Paper](https://arxiv.org/abs/2506.07848v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.19852v1) ¬∑ [GitHub](https://github.com/mit-han-lab/radial-attention) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation | 2025 | [Paper](https://arxiv.org/abs/2509.16500v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.00996) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models | 2025 | [Paper](https://arxiv.org/abs/2505.23656v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image | 2025 | [Paper](https://arxiv.org/abs/2509.04450v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance | 2025 | [Paper](https://arxiv.org/abs/2512.08765) ¬∑ [GitHub](https://github.com/ali-vilab/Wan-Move) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2503.08153) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception | 2025 | [Paper](https://arxiv.org/abs/2508.15720v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) ALIGN-Vid: Aligning Video Diffusion Models with Human Feedback at Scale | 2025 | [Paper](https://arxiv.org/abs/2506.11401) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) VideoDPO: Improving Video Generation with Better Alignment | 2025 | [Paper](https://arxiv.org/abs/2406.06448) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Seedance 1.0: Exploring the Boundaries of Video Generation Models | 2025 | [Paper](https://arxiv.org/abs/2506.09113v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.07280v2) ¬∑ [GitHub](https://github.com/PabloAcuaviva/Gen2Gen) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2506.08009v2) ¬∑ [GitHub](https://github.com/guandeh17/Self-Forcing) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models | 2025 | [Paper](https://arxiv.org/abs/2506.09042v3) ¬∑ [GitHub](https://github.com/nv-tlabs/Cosmos-Drive-Dreams) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.09229v2) ¬∑ [GitHub](https://github.com/deepshwang/crepa) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Consistent Video Editing as Flow-Driven Image-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.07713v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning | 2025 | [Paper](https://arxiv.org/abs/2506.05207v2) ¬∑ [Website](https://follow-your-motion.github.io/) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) PlayerOne: Egocentric World Simulator | 2025 | [Paper](https://arxiv.org/abs/2506.09995v1) ¬∑ [GitHub](https://github.com/yuanpengtu/PlayerOne) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning | 2025 | [Paper](https://arxiv.org/abs/2506.10082v5) ¬∑ [GitHub](https://github.com/cjeen/LoRAEdit) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2506.10568v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning | 2025 | [Paper](https://arxiv.org/abs/2506.10639v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation | 2025 | [Paper](https://arxiv.org/abs/2506.11144v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance | 2025 | [Paper](https://arxiv.org/abs/2506.08456v1) ¬∑ [GitHub](https://github.com/choi403/ALG) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation | 2025 | [Paper](https://arxiv.org/abs/2506.08797v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer | 2025 | [Paper](https://arxiv.org/abs/2506.12847v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Causally Steered Diffusion for Automated Video Counterfactual Generation | 2025 | [Paper](https://arxiv.org/abs/2506.14404v2) ¬∑ [GitHub](https://github.com/nysp78/counterfactual-video-generation) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning | 2025 | [Paper](https://arxiv.org/abs/2506.14827v1) |

### Recent Arxiv Paper

_WIP: will be populated from `arxiv.md` once the arXiv list is complete._

| **Title (Conference / Journal)** | **Year** | **Paper** |
| --- | --- | :---: |


---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üßë‚Äçüíª Contributors

üëè Thanks to these contributors for this excellent workÔºÅ

<a href="https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=cokeshao/Awesome-Multimodal-Token-Compression" />
</a>

## ‚úâÔ∏è Contact

For questions, suggestions, or collaboration opportunities, please feel free to reach out:

‚úâÔ∏è Email:  [chaoyuli@asu.edu](mailto:chaoyuli@asu.edu)

## ‚ú® Star History

[![Star History Chart](https://api.star-history.com/svg?repos=CyL97/Awesome-Video-Generation-Post-Training&type=date&legend=top-left)](https://www.star-history.com/#CyL97/Awesome-Video-Generation-Post-Training&type=date&legend=top-left)
