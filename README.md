<div align=center>

# Awesome Video Generation Post Training

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)
[![arXiv](https://img.shields.io/badge/arXiv-xxxx\.xxxxx-red.svg)]()

[[arXiv]]() [[HuggingFace]]() [[Database]]()

</div>

> **** [[arXiv]]()  
> [Chaoyu Li](https://chaoyuli.com/)<sup>\*,1,‚Ä†</sup>,
> Xiaoyi Gu<sup>2,‚Ä†</sup>,
> Yogesh Kulkarni<sup>1</sup>,
> Eun Woo Im<sup>1</sup>,
> Mohammadmahdi Honarmand<sup>3</sup>,
> Zeyu Wang<sup>4</sup>,
> Juntong Song<sup>5</sup>,
> Fei Du<sup>6</sup>,
> Xilin Jiang<sup>7</sup>,
> Kexin Zheng<sup>8</sup>,
> Tianzhi Li<sup>9</sup>,
> Fei Tao<sup>5</sup>,
> Pooyan Fazli<sup>1</sup>  
>
> <sup>1</sup>Arizona State University ¬∑
> <sup>2</sup>Twitch ¬∑
> <sup>3</sup>Stanford University ¬∑
> <sup>4</sup>eBay ¬∑
> <sup>5</sup>NewsBreak ¬∑
> <sup>6</sup>Microsoft ¬∑
> <sup>7</sup>Columbia University ¬∑
> <sup>8</sup>University of Southern California ¬∑
> <sup>9</sup>Carnegie Mellon University  
>
> <sup>‚Ä†</sup>Equal contribution. <sup>\*</sup>Corresponding author: Chaoyu Li (chaoyuli@asu.edu).

---

> [!IMPORTANT]
> We welcome your help in improving the repository and paper. Please feel free to submit a [pull request]() to:
> 
> - Add a relevant paper not yet included.
>
> - Suggest a more suitable category.
>
> - Update the information.
>
> - Ask for clarification about any content.

---

## üî• News

- **[2026.xx.xx]** The v1 survey is now published! We've also initialized the repository.

## üéØ Motivation

> **Motivation:** .

## üìå Citation

If you find our paper or this resource helpful, please consider cite:

```bibtex
@article{,
  title={},
  author={},
  journal={arXiv preprint arXiv:xxxx.xxxxx},
  year={2026}
}
```

## üìö Contents

- [Awesome Video Generation Post Training](#)
    - [Arxiv Paper](arxiv.md)
    - [Conference Paper](conference.md)

---

### Recent Conference Paper

| **Title (Conference / Journal)** | **Year** | **Links** |
| --- | --- | :---: |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) PanFlow: Decoupled Motion Control for Panoramic Video Generation | 2026 | [Paper](https://arxiv.org/abs/2512.00832v1) ¬∑ [GitHub](https://github.com/chengzhag/PanFlow) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Aligning What Matters: Masked Latent Adaptation for Text-to-Audio-Video Generation | 2025 | [Paper](https://neurips.cc/virtual/2025/loc/mexico-city/poster/118857) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Audio-Sync Video Generation with Multi-Stream Temporal Control | 2025 | [Paper](https://arxiv.org/abs/2506.08003v1) ¬∑ [GitHub](https://github.com/suimuc/MTV_Framework) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.09350v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.03517v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) EchoShot: Multi-Shot Portrait Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.15838) ¬∑ [GitHub](https://github.com/JoHnneyWang/EchoShot) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Frame Context Packing and Drift Prevention in Next-Frame-Prediction Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2504.12626) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Frame In-N-Out: Unbounded Controllable Image-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2505.21491v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) GeoVideo: Introducing Geometric Regularization into Video Generation Model | 2025 | [Paper](https://arxiv.org/abs/2512.03453v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.10858v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Imagine360: Immersive 360 Video Generation from Perspective Anchor | 2025 | [Paper](https://arxiv.org/abs/2412.03552) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Improving Video Generation with Human Feedback | 2025 | [Paper](https://arxiv.org/abs/2501.13918) ¬∑ [GitHub](https://github.com/KlingTeam/VideoAlign) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) MoCha: Towards Movie-Grade Talking Character Generation | 2025 | [Paper](https://arxiv.org/abs/2503.23307) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement | 2025 | [Paper](https://arxiv.org/abs/2506.07848v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.19852v1) ¬∑ [GitHub](https://github.com/mit-han-lab/radial-attention) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation | 2025 | [Paper](https://arxiv.org/abs/2509.16500v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.00996) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models | 2025 | [Paper](https://arxiv.org/abs/2505.23656v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image | 2025 | [Paper](https://arxiv.org/abs/2509.04450v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance | 2025 | [Paper](https://arxiv.org/abs/2512.08765) ¬∑ [GitHub](https://github.com/ali-vilab/Wan-Move) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2503.08153) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception | 2025 | [Paper](https://arxiv.org/abs/2508.15720v1) |
| ![EMNLP](https://img.shields.io/badge/EMNLP-darkorange) VC4VG: Optimizing Video Captions for Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2510.24134) ¬∑ [GitHub](https://github.com/qyr0403/VC4VG) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) AICL: Action In-Context Learning for Video Diffusion Model | 2025 | [Paper](https://dl.acm.org/doi/10.1145/3746027.3754864) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) Improving Identity Preservation in Video Generation with Multi-Branch Models | 2025 | [Paper](https://dl.acm.org/doi/10.1145/3746027.3761990) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) M2PE-DIFF: Music-to-Pose Encoder for Dance Video Generation Leveraging Latent Diffusion Framework. | 2025 | [Paper](https://doi.org/10.1145/3746027.3754808) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation | 2025 | [Paper](https://arxiv.org/html/2508.00782v1) ¬∑ [GitHub](https://github.com/tkpham3105/SpA2V) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation | 2025 | [Paper](https://arxiv.org/abs/2507.05963) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction | 2025 | [Paper](https://arxiv.org/abs/2406.06465) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis | 2025 | [Paper](https://arxiv.org/abs/2507.18569) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Decouple and Track: Benchmarking and Improving Video Diffusion Transformers For Motion Transfer | 2025 | [Paper](https://arxiv.org/abs/2503.17350) ¬∑ [GitHub](https://github.com/Shi-qingyu/DeT) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion | 2025 | [Paper](https://arxiv.org/abs/2504.04010) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) DOLLAR: Few-Step Video Generation via Distillation and Latent Reward Optimization | 2025 | [Paper](https://arxiv.org/abs/2412.15689) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Dual-Expert Consistency Model for Efficient and High-Quality Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.03123) ¬∑ [GitHub](https://github.com/Vchitect/DCM) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization | 2025 | [Paper](https://arxiv.org/abs/2505.02192) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) EfficientMT: Efficient Temporal Adaptation for Motion Transfer in Text-to-Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2503.19369) ¬∑ [GitHub](https://github.com/PrototypeNx/EfficientMT) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Free-Form Motion Control: Controlling the 6D Poses of Camera and Objects in Video Generation | 2025 | [Paper](https://arxiv.org/abs/2501.01425) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) I2VControl: Disentangled and Unified Video Motion Synthesis Control | 2025 | [Paper](https://arxiv.org/abs/2411.17765) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) LayerAnimate: Layer-level Control for Animation | 2025 | [Paper](https://arxiv.org/abs/2501.08295) ¬∑ [GitHub](https://github.com/IamCreateAI/LayerAnimate) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Learning Few-Step Diffusion Models by Trajectory Distribution Matching | 2025 | [Paper](https://arxiv.org/abs/2503.06674) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2507.05678) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Long Context Tuning for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2503.10589) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization | 2025 | [Paper](https://arxiv.org/abs/2503.12689) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MagicMirror: ID-Preserved Video Generation in Video Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2501.03931) ¬∑ [GitHub](https://github.com/dvlab-research/MagicMirror) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance | 2025 | [Paper](https://arxiv.org/abs/2503.16421) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Mobile Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2412.07583) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent | 2025 | [Paper](https://arxiv.org/abs/2502.03207) ¬∑ [GitHub](https://github.com/leoisufa/MotionAgent) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) PersonalVideo: High ID-Fidelity Video Customization without Dynamic and Semantic Degradation | 2025 | [Paper](https://arxiv.org/abs/2411.17048) ¬∑ [GitHub](https://github.com/EchoPluto/PersonalVideo?tab=readme-ov-file) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment | 2025 | [Paper](https://arxiv.org/abs/2502.11079) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Precise Action-to-Video Generation Through Visual Action Prompts | 2025 | [Paper](https://arxiv.org/abs/2508.13104) |


### Recent Arxiv Paper

_WIP: will be populated from `arxiv.md` once the arXiv list is complete._

| **Title** | **Year** | **Links** |
| --- | --- | :---: |


### Datasets

| **Dataset/Benchmark** | **Year** | **Links** |
| --- | --- | :---: |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PNData | 2025 | [Paper](https://arxiv.org/abs/2506.16119) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PairFS-4K | 2025 | [Paper](https://arxiv.org/abs/2505.18078) ¬∑ [GitHub](https://github.com/yisuanwang/DanceTog) ¬∑ [Website](https://dancetog.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DAVID-X | 2025 | [Paper](https://arxiv.org/abs/2506.14827) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Dprim | 2025 | [Paper](https://arxiv.org/abs/2508.20840) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MMVideo | 2025 | [Paper](https://arxiv.org/abs/2507.01938) ¬∑ [GitHub](https://github.com/Tele-AI/CtrlVDiff) ¬∑ [Website](https://tele-ai.github.io/CtrlVDiff) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) CI-VID | 2025 | [Paper](https://arxiv.org/abs/2505.18078) ¬∑ [GitHub](https://github.com/ymju-BAAI/CI-VID) ¬∑ [Dataset](https://huggingface.co/datasets/BAAI/CI-VID) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) TIP-I2V | 2025 | [Paper](https://arxiv.org/abs/2411.04709) ¬∑ [GitHub](https://github.com/WangWenhao0716/TIP-I2V) ¬∑ [Website](https://tip-i2v.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/WenhaoWang/TIP-I2V) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) SynFMC | 2025 | [Paper](https://arxiv.org/abs/2501.01425) ¬∑ [GitHub](https://github.com/FudanCVL/SynFMC) ¬∑ [Website](https://henghuiding.com/SynFMC/) ¬∑ [Dataset](https://huggingface.co/datasets/XinchengShuai/SynFMC) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) WISA-80K | 2025 | [Paper](https://arxiv.org/abs/2503.08153) ¬∑ [GitHub](https://github.com/360CVGroup/WISA) ¬∑ [Website](https://360cvgroup.github.io/WISA/) ¬∑ [Dataset](https://huggingface.co/datasets/qihoo360/WISA-80K)|

| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) VideoUFO | 2025 | [Paper](https://arxiv.org/abs/2508.20840) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) CookGen | 2025 | [Paper](https://arxiv.org/abs/2507.01938) ¬∑ [GitHub](https://github.com/Tele-AI/CtrlVDiff) ¬∑ [Website](https://tele-ai.github.io/CtrlVDiff) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) TalkCuts | 2025 | [Paper](https://arxiv.org/abs/2505.18078) ¬∑ [GitHub](https://github.com/ymju-BAAI/CI-VID) ¬∑ [Dataset](https://huggingface.co/datasets/BAAI/CI-VID) |

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üßë‚Äçüíª Contributors

üëè Thanks to these contributors for this excellent workÔºÅ

<a href="https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=cokeshao/Awesome-Multimodal-Token-Compression" />
</a>

## ‚úâÔ∏è Contact

For questions, suggestions, or collaboration opportunities, please feel free to reach out:

‚úâÔ∏è Email:  [chaoyuli@asu.edu](mailto:chaoyuli@asu.edu)

## ‚ú® Star History

[![Star History Chart](https://api.star-history.com/svg?repos=CyL97/Awesome-Video-Generation-Post-Training&type=date&legend=top-left)](https://www.star-history.com/#CyL97/Awesome-Video-Generation-Post-Training&type=date&legend=top-left)
