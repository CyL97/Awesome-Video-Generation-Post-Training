<div align=center>

# Awesome Video Generation Post Training

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)
[![arXiv](https://img.shields.io/badge/arXiv-xxxx\.xxxxx-red.svg)]()

[[arXiv]]() [[HuggingFace]]() [[Database]]()

</div>

> **** [[arXiv]]()  
> [Chaoyu Li](https://chaoyuli.com/)<sup>\*,1,‚Ä†</sup>,
> Xiaoyi Gu<sup>2,‚Ä†</sup>,
> Yogesh Kulkarni<sup>1</sup>,
> Eun Woo Im<sup>1</sup>,
> Mohammadmahdi Honarmand<sup>3</sup>,
> Zeyu Wang<sup>4</sup>,
> Juntong Song<sup>5</sup>,
> Fei Du<sup>6</sup>,
> Xilin Jiang<sup>7</sup>,
> Kexin Zheng<sup>8</sup>,
> Tianzhi Li<sup>9</sup>,
> Fei Tao<sup>5</sup>,
> Pooyan Fazli<sup>1</sup>  
>
> <sup>1</sup>Arizona State University ¬∑
> <sup>2</sup>Twitch ¬∑
> <sup>3</sup>Stanford University ¬∑
> <sup>4</sup>eBay ¬∑
> <sup>5</sup>NewsBreak ¬∑
> <sup>6</sup>Microsoft ¬∑
> <sup>7</sup>Columbia University ¬∑
> <sup>8</sup>University of Southern California ¬∑
> <sup>9</sup>Carnegie Mellon University  
>
> <sup>‚Ä†</sup>Equal contribution. <sup>\*</sup>Corresponding author: Chaoyu Li (chaoyuli@asu.edu).

---

> [!IMPORTANT]
> We welcome your help in improving the repository and paper. Please feel free to submit a [pull request]() to:
> 
> - Add a relevant paper not yet included.
>
> - Suggest a more suitable category.
>
> - Update the information.
>
> - Ask for clarification about any content.

---

## üî• News

- **[2026.xx.xx]** The v1 survey is now published! We've also initialized the repository.

## üéØ Motivation

> **Motivation:** .

## üìå Citation

If you find our paper or this resource helpful, please consider cite:

```bibtex
@article{,
  title={},
  author={},
  journal={arXiv preprint arXiv:xxxx.xxxxx},
  year={2026}
}
```

## üìö Contents

- [Awesome Video Generation Post Training](#)
    - [Arxiv Paper](arxiv.md)
    - [Conference Paper](conference.md)

---

### Datasets

| **Dataset** | **Year** | **Links** |
| --- | --- | :---: |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) ChronoMagic-Pro | 2024 | [Paper](https://arxiv.org/abs/2406.18522) ¬∑ [GitHub](https://github.com/PKU-YuanGroup/ChronoMagic-Bench) ¬∑ [Website](https://pku-yuangroup.github.io/ChronoMagic-Bench/) ¬∑ [Dataset](https://huggingface.co/datasets/BestWishYsh/ChronoMagic-Pro) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) SafeSora | 2024 | [Paper](https://arxiv.org/abs/2406.14477) ¬∑ [GitHub](https://github.com/PKU-Alignment/safe-sora) ¬∑ [Website](https://sites.google.com/view/safe-sora) ¬∑ [Dataset](https://huggingface.co/datasets/PKU-Alignment/SafeSora) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) TIP-I2V | 2025 | [Paper](https://arxiv.org/abs/2411.04709) ¬∑ [GitHub](https://github.com/WangWenhao0716/TIP-I2V) ¬∑ [Website](https://tip-i2v.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/WenhaoWang/TIP-I2V) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) SynFMC | 2025 | [Paper](https://arxiv.org/abs/2501.01425) ¬∑ [GitHub](https://github.com/FudanCVL/SynFMC) ¬∑ [Website](https://henghuiding.com/SynFMC/) ¬∑ [Dataset](https://huggingface.co/datasets/XinchengShuai/SynFMC) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) CookGen | 2025 | [Paper](https://arxiv.org/abs/2501.06173) ¬∑ [Website](https://videoauteur.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/lambertxiao/CookGen_youcook2) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) HOIGen-1M | 2025 | [Paper](https://arxiv.org/abs/2503.23715) ¬∑ [Website](https://liuqi-creat.github.io/HOIGen.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/HOIGen/HOIGen-1M) |
| ![ICML](https://img.shields.io/badge/ICML-indigo) PhyWorld | 2025 | [Paper](https://arxiv.org/abs/2411.02385) ¬∑ [GitHub](https://github.com/phyworld/phyworld) ¬∑ [Website](https://phyworld.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/magicr/phyworld) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) WISA-80K | 2025 | [Paper](https://arxiv.org/abs/2503.08153) ¬∑ [GitHub](https://github.com/360CVGroup/WISA) ¬∑ [Website](https://360cvgroup.github.io/WISA/) ¬∑ [Dataset](https://huggingface.co/datasets/qihoo360/WISA-80K) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) VideoUFO | 2025 | [Paper](https://arxiv.org/abs/2503.01739) ¬∑ [GitHub](https://github.com/WangWenhao0716/BenchUFO) ¬∑ [Dataset](https://huggingface.co/datasets/WenhaoWang/VideoUFO) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) TalkCuts | 2025 | [Paper](https://arxiv.org/abs/2510.07249) ¬∑ [GitHub](https://github.com/UMass-Embodied-AGI/TalkCuts) ¬∑ [Website](https://talkcuts.github.io/) ¬∑ [Dataset](https://docs.google.com/forms/d/e/1FAIpQLSfapK7pqgyrcCaOxJn8yQc79AYaq1DOvJzL0-VZnLiA3CvpyQ/viewform) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) EgoVid-5M | 2025 | [Paper](https://arxiv.org/abs/2411.08380) ¬∑ [GitHub](https://github.com/JeffWang987/EgoVid) ¬∑ [Website](https://egovid.github.io/) ¬∑ [Dataset](https://modelscope.cn/datasets/iic/EgoVid/) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) OpenS2V-5M | 2025 | [Paper](https://arxiv.org/abs/2505.20292) ¬∑ [GitHub](https://github.com/PKU-YuanGroup/OpenS2V-Nexus) ¬∑ [Website](https://pku-yuangroup.github.io/OpenS2V-Nexus/) ¬∑ [Dataset](https://huggingface.co/datasets/BestWishYsh/OpenS2V-5M) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) CI-VID | 2025 | [Paper](https://arxiv.org/abs/2505.18078) ¬∑ [GitHub](https://github.com/ymju-BAAI/CI-VID) ¬∑ [Dataset](https://huggingface.co/datasets/BAAI/CI-VID) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PNData | 2025 | [Paper](https://arxiv.org/abs/2506.16119) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PairFS-4K | 2025 | [Paper](https://arxiv.org/abs/2505.18078) ¬∑ [GitHub](https://github.com/yisuanwang/DanceTog) ¬∑ [Website](https://dancetog.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DAVID-X | 2025 | [Paper](https://arxiv.org/abs/2506.14827) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Dprim | 2025 | [Paper](https://arxiv.org/abs/2508.20840) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MMVideo | 2025 | [Paper](https://arxiv.org/abs/2507.01938) ¬∑ [GitHub](https://github.com/Tele-AI/CtrlVDiff) ¬∑ [Website](https://tele-ai.github.io/CtrlVDiff) |
| ![PMLR](https://img.shields.io/badge/PMLR-gold) GRADEO-Instruct | 2025 | [Paper](https://arxiv.org/abs/2503.02341) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) OpenHumanVid | 2025 | [Paper](https://arxiv.org/abs/2412.00115) ¬∑ [GitHub](https://github.com/fudan-generative-vision/OpenHumanVid) ¬∑ [Website](https://fudan-generative-vision.github.io/OpenHumanVid/#/) |

### Benchmarks

| **Benchmark** | **Year** | **Links** |
| --- | --- | :---: |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) FETV | 2023 | [Paper](https://arxiv.org/abs/2311.01813) ¬∑ [GitHub](https://github.com/llyx97/FETV) ¬∑ [Dataset](https://huggingface.co/datasets/lyx97/FETV) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) StoryBench | 2023 | [Paper](https://arxiv.org/abs/2308.11606) ¬∑ [GitHub](https://github.com/google/storybench) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) ChronoMagic-Bench | 2024 | [Paper](https://arxiv.org/abs/2406.18522) ¬∑ [GitHub](https://github.com/PKU-YuanGroup/ChronoMagic-Bench) ¬∑ [Website](https://pku-yuangroup.github.io/ChronoMagic-Bench/) ¬∑ [Dataset](https://huggingface.co/datasets/BestWishYsh/ChronoMagic-Bench) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) EvalCrafter | 2024 | [Paper](https://arxiv.org/abs/2310.11440) ¬∑ [GitHub](https://github.com/EvalCrafter/EvalCrafter) ¬∑ [Website](https://evalcrafter.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/RaphaelLiu/EvalCrafter_T2V_Dataset) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) VBench | 2024 | [Paper](https://arxiv.org/abs/2311.17982) ¬∑ [GitHub](https://github.com/Vchitect/VBench) ¬∑ [Website](https://vchitect.github.io/VBench-project/) ¬∑ [Dataset](https://drive.google.com/drive/folders/1on66fnZ8atRoLDimcAXMxSwRxqN8_0yS) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) T2VSafetyBench | 2024 | [Paper](https://arxiv.org/abs/2407.05965) ¬∑ [GitHub](https://github.com/yibo-miao/T2VSafetyBench) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MTBench | 2025 | [Paper](https://arxiv.org/abs/2503.17350) ¬∑ [GitHub](https://github.com/Shi-qingyu/DeT) ¬∑ [Website](https://shi-qingyu.github.io/DeT.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/QingyuShi/MTBench) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) FiVE | 2025 | [Paper](https://arxiv.org/abs/2503.13684) ¬∑ [GitHub](https://github.com/MinghanLi/FiVE-Bench) ¬∑ [Website](https://sites.google.com/view/five-benchmark) ¬∑ [Dataset](https://huggingface.co/datasets/LIMinghan/FiVE-Fine-Grained-Video-Editing-Benchmark) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) VEG-Bench | 2025 | [Paper](https://arxiv.org/abs/2503.14350) ¬∑ [GitHub](https://github.com/Yui010206/VEGGIE-VidEdit/) ¬∑ [Website](https://veggie-gen.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/Shoubin/VEGGIE-Bench) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) VMBench | 2025 | [Paper](https://arxiv.org/abs/2503.10076) ¬∑ [GitHub](https://github.com/AMAP-ML/VMBench) ¬∑ [Website](https://amap-ml.github.io/VMBench-Website/) ¬∑ [Dataset](https://huggingface.co/GD-ML/VMBench) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) T2V-CompBench | 2025 | [Paper](https://arxiv.org/abs/2407.14505) ¬∑ [GitHub](https://github.com/KaiyueSun98/T2V-CompBench) ¬∑ [Website](https://t2v-compbench-2025.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/Kaiyue/T2V-CompBench-Videos) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) StoryEval | 2025 | [Paper](https://arxiv.org/abs/2412.16211) ¬∑ [GitHub](https://github.com/ypwang61/StoryEval) ¬∑ [Website](https://ypwang61.github.io/project/StoryEval/) ¬∑ [Dataset](https://drive.google.com/drive/u/0/folders/1fpvSOmPMQ0jcYyZJ0G6rweKKe-2fbATn) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) MC-Bench | 2025 | [Paper](https://arxiv.org/abs/2505.20287) ¬∑ [GitHub](https://github.com/HiDream-ai/MotionPro) ¬∑ [Website](https://zhw-zhang.github.io/MotionPro-page/) ¬∑ [Dataset](https://huggingface.co/HiDream-ai/MotionPro/tree/main) |
| ![CVPR](https://img.shields.io/badge/CVPR-brightgreen) Video-Bench | 2025 | [Paper](https://arxiv.org/abs/2504.04907) ¬∑ [GitHub](https://github.com/Video-Bench/Video-Bench) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) MJ-BENCH-VIDEO | 2025 | [Paper](https://arxiv.org/abs/2502.01719) ¬∑ [GitHub](https://github.com/aiming-lab/MJ-Video) ¬∑ [Website](https://aiming-lab.github.io/MJ-VIDEO.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/MJ-Bench/MJ-BENCH-VIDEO) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) OpenS2V-Eval | 2025 | [Paper](https://arxiv.org/abs/2505.20292) ¬∑ [GitHub](https://github.com/PKU-YuanGroup/OpenS2V-Nexus) ¬∑ [Website](https://pku-yuangroup.github.io/OpenS2V-Nexus/) ¬∑ [Dataset](https://huggingface.co/datasets/BestWishYsh/OpenS2V-Eval) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) VideoGen-RewardBench | 2025 | [Paper](https://arxiv.org/abs/2501.13918) ¬∑ [GitHub](https://github.com/KlingTeam/VideoAlign) ¬∑ [Website](https://gongyeliu.github.io/videoalign/) ¬∑ [Dataset](https://huggingface.co/datasets/KlingTeam/VideoGen-RewardBench) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) AIGC-LipSync | 2025 | [Paper](https://arxiv.org/abs/2505.21448) ¬∑ [Website](https://ziqiaopeng.github.io/OmniSync/) ¬∑ [Dataset](https://huggingface.co/datasets/ZiqiaoPeng/AIGC_LipSync_Benchmark) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) WorldModelBench | 2025 | [Paper](https://arxiv.org/abs/2502.20694) ¬∑ [GitHub](https://github.com/WorldModelBench-Team/WorldModelBench/tree/main?tab=readme-ov-file#evaluation) ¬∑ [Website](https://worldmodelbench-team.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/Efficient-Large-Model/worldmodelbench) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) VIP-200K | 2025 | [Paper](https://dl.acm.org/doi/10.1145/3746027.3761987) ¬∑ [Website](https://hidream-ai.github.io/ipvg-challenge.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/HiDream-ai/VIP-200K) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) RGCD | 2025 | [Paper](https://arxiv.org/abs/2506.23852) ¬∑ [GitHub](https://github.com/IntMeGroup/RGC-VQA) ¬∑ [Dataset](https://pan.baidu.com/share/init?surl=ebpoA3-DRi3XhT6d68Bf4A&pwd=hab7) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) RecipeGen | 2025 | [Paper](https://arxiv.org/abs/2506.06733) ¬∑ [Dataset](https://huggingface.co/datasets/RUOXUAN123/RecipeGen) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) HVEval | 2025 | [Paper](https://dl.acm.org/doi/10.1145/3746027.3758299) |
| ![EMNLP](https://img.shields.io/badge/EMNLP-darkorange) Doc2Present | 2025 | [Paper](https://arxiv.org/abs/2507.04036) ¬∑ [GitHub](https://github.com/AIGeeksGroup/PresentAgent) ¬∑ [Dataset](https://huggingface.co/datasets/AIGeeksGroup/Doc2Present) |
| ![ACL](https://img.shields.io/badge/ACL-%23FF6347) VidCapBench | 2025 | [Paper](https://arxiv.org/abs/2502.12782) ¬∑ [GitHub](https://github.com/VidCapBench/VidCapBench) ¬∑ [Dataset](https://huggingface.co/datasets/VidCapBench/VidCapBench) |
| ![ICLR](https://img.shields.io/badge/ICLR-teal) VideoPhy | 2025 | [Paper](https://openreview.net/forum?id=9D2QvO1uWj) ¬∑ [GitHub](https://github.com/Hritikbansal/videophy) ¬∑ [Dataset](https://huggingface.co/datasets/videophysics/videophy_test_public) |
| ![ICML](https://img.shields.io/badge/ICML-indigo) PhyGenBench | 2025 | [Paper](https://arxiv.org/abs/2410.05363) ¬∑ [GitHub](https://github.com/OpenGVLab/PhyGenBench) ¬∑ [Website](https://phygenbench123.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/phygenbench/phygenbench) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Paper2Video | 2025 | [Paper](https://arxiv.org/abs/2510.05096) ¬∑ [GitHub](https://github.com/showlab/Paper2Video) ¬∑ [Website](https://showlab.github.io/Paper2Video/) ¬∑ [Dataset](https://huggingface.co/datasets/ZaynZhu/Paper2Video) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PhyWorldBench | 2025 | [Paper](https://arxiv.org/abs/2507.13428) ¬∑ [GitHub](https://github.com/g-jing/phy-world-bench) ¬∑ [Dataset](https://huggingface.co/datasets/phyworldbench/phyworldbench) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SafeMVDrive | 2025 | [Paper](https://arxiv.org/abs/2505.17727) ¬∑ [GitHub](https://github.com/zhoujiawei3/SafeMVDrive) ¬∑ [Website](https://zhoujiawei3.github.io/SafeMVDrive/) ¬∑ [Dataset](https://huggingface.co/datasets/JiaweiZhou/SafeMVDrive) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SeqBench | 2025 | [Paper](https://arxiv.org/abs/2510.13042) ¬∑ [GitHub](https://github.com/TangZhengxu/SeqBench-Benchmarking-Sequential-Narrative-Generation-in-Text-to-Video-Models) ¬∑ [Website](https://videobench.github.io/SeqBench.github.io/) ¬∑ [Dataset](https://huggingface.co/datasets/AcmmmVideobench/Acmmm2025_video_benchmark) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) VideoVerse | 2025 | [Paper](https://arxiv.org/abs/2510.08398) ¬∑ [GitHub](https://github.com/Zeqing-Wang/VideoVerse) ¬∑ [Website](https://www.naptmn.cn/Homepage_of_VideoVerse/) ¬∑ [Dataset](https://huggingface.co/datasets/NNaptmn/VideoVerse) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) AIGVE-60K | 2025 | [Paper](https://arxiv.org/abs/2505.12098) ¬∑ [GitHub](https://github.com/IntMeGroup/LOVE) ¬∑ [Dataset](https://huggingface.co/datasets/anonymousdb/AIGVE-60K) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MMMC | 2025 | [Paper](https://arxiv.org/abs/2510.01174) ¬∑ [GitHub](https://github.com/showlab/Code2Video/) ¬∑ [Website](https://showlab.github.io/Code2Video/) ¬∑ [Dataset](https://huggingface.co/datasets/YanzheChen/MMMC) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DynamicEval | 2025 | [Paper](https://arxiv.org/abs/2510.07441) ¬∑ [GitHub](https://github.com/nithincbabu7/DynamicEval) ¬∑ [Website](https://nithincbabu7.github.io/DynamicEval/) ¬∑ [Dataset](https://huggingface.co/datasets/hexmSeeU/dynamiceval) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) V-ReasonBench | 2025 | [Paper](https://arxiv.org/abs/2511.16668) ¬∑ [GitHub](https://github.com/yangluo7/V-ReasonBench) ¬∑ [Website](https://oahzxl.github.io/VReasonBench/) ¬∑ [Dataset](https://huggingface.co/datasets/yangluo7/V-ReasonBench) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) RGCD (alternate) | 2025 | [Paper](https://arxiv.org/abs/2506.23852) ¬∑ [GitHub](https://github.com/IntMeGroup/RGC-VQA) ¬∑ [Dataset](https://pan.baidu.com/share/init?surl=ebpoA3-DRi3XhT6d68Bf4A&pwd=hab7) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) GenVidBench | 2025 | [Paper](https://arxiv.org/abs/2511.13897) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DIVE | 2025 | [Paper](https://arxiv.org/abs/2505.19901) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) UI2V-Bench | 2025 | [Paper](https://arxiv.org/abs/2509.24427) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PhysVidBench | 2025 | [Paper](https://arxiv.org/abs/2507.15824) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SurgVeo | 2025 | [Paper](https://arxiv.org/abs/2511.01775) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LoCoT2V-Bench | 2025 | [Paper](https://arxiv.org/abs/2510.26412) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PAI-Bench | 2025 | [Paper](https://arxiv.org/abs/2512.01989) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MEve | 2025 | [Paper](https://arxiv.org/abs/2510.03049) |
| ![WACV](https://img.shields.io/badge/WACV-dodgerblue) GeneVA | 2026 | [Paper](https://arxiv.org/abs/2509.08818) ¬∑ [Website](https://www.immersivecomputinglab.org/publication/geneva/) ¬∑ [Dataset](https://ai-generated-videos-icl.s3.us-east-1.amazonaws.com) |


### Recent Conference Paper

| **Title (Conference / Journal)** | **Year** | **Links** |
| --- | --- | :---: |
| ![AAAI](https://img.shields.io/badge/AAAI-cyan) PanFlow: Decoupled Motion Control for Panoramic Video Generation | 2026 | [Paper](https://arxiv.org/abs/2512.00832v1) ¬∑ [GitHub](https://github.com/chengzhag/PanFlow) ¬∑ [Website](https://chengzhag.github.io/publication/panflow/)|
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Aligning What Matters: Masked Latent Adaptation for Text-to-Audio-Video Generation | 2025 | [Paper](https://neurips.cc/virtual/2025/loc/mexico-city/poster/118857)|
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Audio-Sync Video Generation with Multi-Stream Temporal Control | 2025 | [Paper](https://arxiv.org/abs/2506.08003v1) ¬∑ [GitHub](https://github.com/suimuc/MTV_Framework) ¬∑ [Website](https://hjzheng.net/projects/MTV/)|
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.09350v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.03517v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) EchoShot: Multi-Shot Portrait Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.15838) ¬∑ [GitHub](https://github.com/JoHnneyWang/EchoShot) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Frame Context Packing and Drift Prevention in Next-Frame-Prediction Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2504.12626) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Frame In-N-Out: Unbounded Controllable Image-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2505.21491v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) GeoVideo: Introducing Geometric Regularization into Video Generation Model | 2025 | [Paper](https://arxiv.org/abs/2512.03453v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation | 2025 | [Paper](https://arxiv.org/abs/2508.10858v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Imagine360: Immersive 360 Video Generation from Perspective Anchor | 2025 | [Paper](https://arxiv.org/abs/2412.03552) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Improving Video Generation with Human Feedback | 2025 | [Paper](https://arxiv.org/abs/2501.13918) ¬∑ [GitHub](https://github.com/KlingTeam/VideoAlign) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) MoCha: Towards Movie-Grade Talking Character Generation | 2025 | [Paper](https://arxiv.org/abs/2503.23307) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement | 2025 | [Paper](https://arxiv.org/abs/2506.07848v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.19852v1) ¬∑ [GitHub](https://github.com/mit-han-lab/radial-attention) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation | 2025 | [Paper](https://arxiv.org/abs/2509.16500v2) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2506.00996) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models | 2025 | [Paper](https://arxiv.org/abs/2505.23656v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image | 2025 | [Paper](https://arxiv.org/abs/2509.04450v1) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance | 2025 | [Paper](https://arxiv.org/abs/2512.08765) ¬∑ [GitHub](https://github.com/ali-vilab/Wan-Move) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2503.08153) |
| ![NeurIPS](https://img.shields.io/badge/NeurIPS-blue) WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception | 2025 | [Paper](https://arxiv.org/abs/2508.15720v1) |
| ![EMNLP](https://img.shields.io/badge/EMNLP-darkorange) VC4VG: Optimizing Video Captions for Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2510.24134) ¬∑ [GitHub](https://github.com/qyr0403/VC4VG) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) AICL: Action In-Context Learning for Video Diffusion Model | 2025 | [Paper](https://dl.acm.org/doi/10.1145/3746027.3754864) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) Improving Identity Preservation in Video Generation with Multi-Branch Models | 2025 | [Paper](https://dl.acm.org/doi/10.1145/3746027.3761990) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) M2PE-DIFF: Music-to-Pose Encoder for Dance Video Generation Leveraging Latent Diffusion Framework. | 2025 | [Paper](https://doi.org/10.1145/3746027.3754808) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation | 2025 | [Paper](https://arxiv.org/html/2508.00782v1) ¬∑ [GitHub](https://github.com/tkpham3105/SpA2V) |
| ![ACM MM](https://img.shields.io/badge/ACM_MM-darkgray) Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation | 2025 | [Paper](https://arxiv.org/abs/2507.05963) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction | 2025 | [Paper](https://arxiv.org/abs/2406.06465) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis | 2025 | [Paper](https://arxiv.org/abs/2507.18569) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Decouple and Track: Benchmarking and Improving Video Diffusion Transformers For Motion Transfer | 2025 | [Paper](https://arxiv.org/abs/2503.17350) ¬∑ [GitHub](https://github.com/Shi-qingyu/DeT) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion | 2025 | [Paper](https://arxiv.org/abs/2504.04010) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) DOLLAR: Few-Step Video Generation via Distillation and Latent Reward Optimization | 2025 | [Paper](https://arxiv.org/abs/2412.15689) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Dual-Expert Consistency Model for Efficient and High-Quality Video Generation | 2025 | [Paper](https://arxiv.org/abs/2506.03123) ¬∑ [GitHub](https://github.com/Vchitect/DCM) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization | 2025 | [Paper](https://arxiv.org/abs/2505.02192) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) EfficientMT: Efficient Temporal Adaptation for Motion Transfer in Text-to-Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2503.19369) ¬∑ [GitHub](https://github.com/PrototypeNx/EfficientMT) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Free-Form Motion Control: Controlling the 6D Poses of Camera and Objects in Video Generation | 2025 | [Paper](https://arxiv.org/abs/2501.01425) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) I2VControl: Disentangled and Unified Video Motion Synthesis Control | 2025 | [Paper](https://arxiv.org/abs/2411.17765) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) LayerAnimate: Layer-level Control for Animation | 2025 | [Paper](https://arxiv.org/abs/2501.08295) ¬∑ [GitHub](https://github.com/IamCreateAI/LayerAnimate) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Learning Few-Step Diffusion Models by Trajectory Distribution Matching | 2025 | [Paper](https://arxiv.org/abs/2503.06674) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2507.05678) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Long Context Tuning for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2503.10589) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization | 2025 | [Paper](https://arxiv.org/abs/2503.12689) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MagicMirror: ID-Preserved Video Generation in Video Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2501.03931) ¬∑ [GitHub](https://github.com/dvlab-research/MagicMirror) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance | 2025 | [Paper](https://arxiv.org/abs/2503.16421) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Mobile Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2412.07583) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent | 2025 | [Paper](https://arxiv.org/abs/2502.03207) ¬∑ [GitHub](https://github.com/leoisufa/MotionAgent) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) PersonalVideo: High ID-Fidelity Video Customization without Dynamic and Semantic Degradation | 2025 | [Paper](https://arxiv.org/abs/2411.17048) ¬∑ [GitHub](https://github.com/EchoPluto/PersonalVideo?tab=readme-ov-file) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment | 2025 | [Paper](https://arxiv.org/abs/2502.11079) |
| ![ICCV](https://img.shields.io/badge/ICCV-green) Precise Action-to-Video Generation Through Visual Action Prompts | 2025 | [Paper](https://arxiv.org/abs/2508.13104) |


### Recent Arxiv Paper

_WIP: will be populated from `arxiv.md` once the arXiv list is complete._

| **Title** | **Year** | **Links** |
| --- | --- | :---: |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation | 2025 | [Paper](https://arxiv.org/abs/2512.03621v1) ¬∑ [Website](https://recamdriving.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) In-Context Sync-LoRA for Portrait Video Editing | 2025 | [Paper](https://arxiv.org/abs/2512.03013v1) ¬∑ [Website](https://sagipolaczek.github.io/Sync-LoRA/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Progressive Image Restoration via Text-Conditioned Video Generation | 2025 | [Paper](https://arxiv.org/abs/2512.02273v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Generative Video Motion Editing with 3D Point Tracks | 2025 | [Paper](https://arxiv.org/abs/2512.02015v1) ¬∑ [Website](https://edit-by-track.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation | 2025 | [Paper](https://arxiv.org/abs/2512.01960v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models | 2025 | [Paper](https://arxiv.org/abs/2512.01686v1) ¬∑ [Website](https://yj7082126.github.io/dreamingcomics/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Low-Bitrate Video Compression through Semantic-Conditioned Diffusion | 2025 | [Paper](https://arxiv.org/abs/2512.00408v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement | 2025 | [Paper](https://arxiv.org/abs/2511.23475v1) ¬∑ [Website](https://hkust-c4g.github.io/AnyTalker-homepage/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.22973v1) ¬∑ [Website](https://ziplab.co/BlockVid) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) WorldWander: Bridging Egocentric and Exocentric Worlds in Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.22098v1) ¬∑ [GitHub](https://github.com/showlab/WorldWander) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion | 2025 | [Paper](https://arxiv.org/abs/2511.21129v1) ¬∑ [Website](https://tele-ai.github.io/CtrlVDiff/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MotionV2V: Editing Motion in a Video | 2025 | [Paper](https://arxiv.org/abs/2511.20640v1) ¬∑ [Website](https://ryanndagreat.github.io/MotionV2V/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis | 2025 | [Paper](https://arxiv.org/abs/2511.20186v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation | 2025 | [Paper](https://arxiv.org/abs/2511.19320v1) ¬∑ [Website](https://mcg-nju.github.io/steadydancer-web/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.19049v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) View-Consistent Diffusion Representations for 3D-Consistent Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.18991v1) ¬∑ [Website](https://danier97.github.io/ViCoDR/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Eevee: Towards Close-up High-resolution Video-based Virtual Try-on | 2025 | [Paper](https://arxiv.org/abs/2511.18957v1) ¬∑ [GitHub](https://github.com/AMAP-ML/Eevee) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution | 2025 | [Paper](https://arxiv.org/abs/2511.18786v1) ¬∑ [Website](https://jychen9811.github.io/STCDiT_page/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Point-to-Point: Sparse Motion Guidance for Controllable Video Editing | 2025 | [Paper](https://arxiv.org/abs/2511.18277v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) CamC2V: Context-aware Controllable Video Generation | 2025 | [Paper](https://arxiv.org/abs/2504.06022v2) ¬∑ [GitHub](https://github.com/LDenninger/CamC2V) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MultiShotMaster: A Controllable Multi-Shot Video Generation Framework | 2025 | [Paper](https://arxiv.org/abs/2512.03041v1) ¬∑ [GitHub](https://qinghew.github.io/MultiShotMaster/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization | 2025 | [Paper](https://arxiv.org/abs/2512.02933v2) ¬∑ [GitHub](https://cz-5f.github.io/LoVoRA.github.io/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Taming Camera-Controlled Video Generation with Verifiable Geometry Reward | 2025 | [Paper](https://arxiv.org/abs/2512.02870v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) IC-World: In-Context Generation for Shared World Modeling | 2025 | [Paper](https://arxiv.org/abs/2512.02793v1) ¬∑ [GitHub](https://github.com/wufan-cse/IC-World) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Make Objects Move Slower Than in Reality | 2025 | [Paper](https://arxiv.org/abs/2512.02016v1) ¬∑ [GitHub](https://gravity-eval.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards | 2025 | [Paper](https://arxiv.org/abs/2512.00425v1) ¬∑ [GitHub](https://cvlab-stonybrook.github.io/NewtonRewards/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) InstanceV: Instance-Level Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.23146v1) ¬∑ [GitHub](https://aliothchen.github.io/projects/InstanceV/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning | 2025 | [Paper](https://arxiv.org/abs/2511.22974v1) ¬∑ [GitHub](https://github.com/QiushiYang/McSc) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) One-to-All Animation: Alignment-Free Character Animation and Image Pose Transfer | 2025 | [Paper](https://arxiv.org/abs/2511.22940v2) ¬∑ [GitHub](https://ssj9596.github.io/one-to-all-animation-project/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training | 2025 | [Paper](https://arxiv.org/abs/2511.21592v1) ¬∑ [GitHub](https://xavihart.github.io/mogan/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Video Generation Models Are Good Latent Reward Models | 2025 | [Paper](https://arxiv.org/abs/2511.21541v1) ¬∑ [GitHub](https://kululumi.github.io/PRFL/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models | 2025 | [Paper](https://arxiv.org/abs/2511.20629v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Growing with the Generator: Self-paced GRPO for Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.19356v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation | 2025 | [Paper](https://arxiv.org/abs/2511.18919v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses | 2025 | [Paper](https://arxiv.org/abs/2511.18173v1) ¬∑ [Website](https://cvg-bonn.github.io/EgoControl/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.17844v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Show Me: Unifying Instructional Image and Video Generation with Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2511.17839v1) ¬∑ [GitHub](https://yujiangpu20.github.io/showme/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO | 2025 | [Paper](https://arxiv.org/abs/2511.16669v2) ¬∑ [GitHub](https://video-as-answer.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) First Frame Is the Place to Go for Video Content Customization | 2025 | [Paper](https://arxiv.org/abs/2511.15700v1) ¬∑ [Website](http://firstframego.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.14993v2) ¬∑ [GitHub](https://github.com/kandinskylab/kandinsky-5) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models | 2025 | [Paper](https://arxiv.org/abs/2511.13704v1) ¬∑ [GitHub](https://haroldchen19.github.io/TiViBench-Page/) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models | 2025 | [Paper](https://arxiv.org/abs/2511.12099v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.12072v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) LiteAttention: A Temporal Sparse Attention for Diffusion Transformers | 2025 | [Paper](https://arxiv.org/abs/2511.11062v1) ¬∑ [GitHub](https://github.com/moonmath-ai/LiteAttention) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation | 2025 | [Paper](https://arxiv.org/abs/2511.11002v1) ¬∑ [Website](https://zane-zyqiu.github.io/EmoVid) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Video Text Preservation with Synthetic Text-Rich Videos | 2025 | [Paper](https://arxiv.org/abs/2511.05573v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) Neodragon: Mobile Video Generation using Diffusion Transformer | 2025 | [Paper](https://arxiv.org/abs/2511.06055v1) ¬∑ [GitHub](https://qualcomm-ai-research.github.io/neodragon) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.04317v1) ¬∑ [GitHub](https://rise-t2v.github.io) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection | 2025 | [Paper](https://arxiv.org/abs/2511.03997v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions | 2025 | [Paper](https://arxiv.org/abs/2511.03334v1) |
| ![arXiv](https://img.shields.io/badge/arXiv-red) ID-Crafter: VLM-Grounded Online RL for Compositional Multi-Subject Video Generation | 2025 | [Paper](https://arxiv.org/abs/2511.00511v3) ¬∑ [GitHub](https://angericky.github.io/ID-Crafter/) |

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üßë‚Äçüíª Contributors

üëè Thanks to these contributors for this excellent workÔºÅ

<a href="https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=cokeshao/Awesome-Multimodal-Token-Compression" />
</a>

## ‚úâÔ∏è Contact

For questions, suggestions, or collaboration opportunities, please feel free to reach out:

‚úâÔ∏è Email:  [chaoyuli@asu.edu](mailto:chaoyuli@asu.edu)

## ‚ú® Star History

[![Star History Chart](https://api.star-history.com/svg?repos=CyL97/Awesome-Video-Generation-Post-Training&type=date&legend=top-left)](https://www.star-history.com/#CyL97/Awesome-Video-Generation-Post-Training&type=date&legend=top-left)
